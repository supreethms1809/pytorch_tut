{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+852RBTF1H03PyFyGlJxn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supreethms1809/pytorch_tut/blob/main/pytorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pbAyF54jR1_v",
        "outputId": "4ddef69f-f430-4a05-b6ec-916d83f24ec2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib as plt\n",
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
        "\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression"
      ],
      "metadata": {
        "id": "61A45oFdpdoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we have created some data X and y using a relation.\n",
        "We want to estimate the weight and bias or the function using a machine learning model"
      ],
      "metadata": {
        "id": "lOtsm3I0qegW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data (preparing and loading)\n",
        "\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "#create some data\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "X = torch.arange(start,end,step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "X[:10], y[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvPfh52fpGLF",
        "outputId": "cb218944-095b-4657-a05c-034c9037d540"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data\n",
        "train_len = int(0.8*len(X))\n",
        "X_train = X[:train_len]\n",
        "y_train = y[:train_len]\n",
        "\n",
        "X_test = X[train_len:]\n",
        "y_test = y[train_len:]\n",
        "\n"
      ],
      "metadata": {
        "id": "MkH7jRH_q0-S"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_prediction(train_data = X_train,\n",
        "                    train_labels = y_train,\n",
        "                    test_data = X_test,\n",
        "                    test_labels = y_test,\n",
        "                    predictions=None):\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.scatter(train_data,train_labels,c=\"b\",s=4,label=\"Training data\")\n",
        "  plt.scatter(test_data,test_labels,c=\"g\",s=4,label=\"Test data\")\n",
        "\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data,predictions,c=\"r\",s=4,label=\"Predictions\")\n",
        "\n",
        "  plt.legend(prop={\"size\": 14})\n",
        "\n",
        "plot_prediction();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "pqesoVXYruyN",
        "outputId": "83e2172f-2878-49a2-cb8d-d7021ea3bb18"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class LinearRegressionModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.weights = nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n",
        "\n",
        "  def forward(self,x:torch.Tensor) -> torch.Tensor:\n",
        "    return self.weights * x + self.bias\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "predictions = LinearRegressionModel()\n",
        "print(list(predictions.parameters()))\n",
        "print(predictions.state_dict())\n",
        "predictions(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGuGSKpHvIjL",
        "outputId": "2d2f9ed1-5170-4551-a978-2fb37a1f3f56"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([0.3367], requires_grad=True), Parameter containing:\n",
            "tensor([0.1288], requires_grad=True)]\n",
            "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "#setup a loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "c_loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "#setup an optimizer\n",
        "optimizer = torch.optim.SGD(params=predictions.parameters(),\n",
        "                            lr = 0.00001,\n",
        "                            )\n",
        "\n"
      ],
      "metadata": {
        "id": "4cZjBqsE660m"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a training loop and testing loop in pytorch\n",
        "epochs = 10000\n",
        "\n",
        "#Tracking the values\n",
        "epoch_count = []\n",
        "loss_values = []\n",
        "test_loss_value = []\n",
        "\n",
        "#Step 0\n",
        "for epoch in range(epochs):\n",
        "   # set the training mode\n",
        "   predictions.train()\n",
        "\n",
        "   #Forward pass\n",
        "   y_pred = predictions(X_train)\n",
        "\n",
        "   # Calculate the loss\n",
        "   loss = loss_fn(y_pred,y_train)\n",
        "\n",
        "   # Optimizer zero grad\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   #Perform backpropogation\n",
        "   loss.backward()\n",
        "\n",
        "   #setuo the optimizer\n",
        "   optimizer.step()\n",
        "   epoch_count.append(epoch)\n",
        "   loss_values.append(loss)\n",
        "   if epoch%10 == 0:\n",
        "    print(f'Train-->Loss: {loss}, epoch: {epoch}')\n",
        "\n",
        "   # Testing\n",
        "    predictions.eval()\n",
        "    with torch.no_grad():\n",
        "      y_test_pred = predictions(X_test)\n",
        "      test_loss = loss_fn(y_test_pred,y_test)\n",
        "      test_loss_value.append(test_loss)\n",
        "    if epoch%10 == 0:\n",
        "      print(f'Test --> Loss: {loss}, epoch: {epoch}')\n",
        "\n",
        "predictions.state_dict()\n",
        "#predictions.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQBFn1X-Nr1j",
        "outputId": "30cea727-f0a7-4306-9389-45b2ed56bf13"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-->Loss: 0.31288138031959534, epoch: 0\n",
            "Test --> Loss: 0.31288138031959534, epoch: 0\n",
            "Train-->Loss: 0.3127661347389221, epoch: 10\n",
            "Test --> Loss: 0.3127661347389221, epoch: 10\n",
            "Train-->Loss: 0.3126509487628937, epoch: 20\n",
            "Test --> Loss: 0.3126509487628937, epoch: 20\n",
            "Train-->Loss: 0.31253570318222046, epoch: 30\n",
            "Test --> Loss: 0.31253570318222046, epoch: 30\n",
            "Train-->Loss: 0.312420517206192, epoch: 40\n",
            "Test --> Loss: 0.312420517206192, epoch: 40\n",
            "Train-->Loss: 0.3123053312301636, epoch: 50\n",
            "Test --> Loss: 0.3123053312301636, epoch: 50\n",
            "Train-->Loss: 0.31219008564949036, epoch: 60\n",
            "Test --> Loss: 0.31219008564949036, epoch: 60\n",
            "Train-->Loss: 0.3120748698711395, epoch: 70\n",
            "Test --> Loss: 0.3120748698711395, epoch: 70\n",
            "Train-->Loss: 0.3119596540927887, epoch: 80\n",
            "Test --> Loss: 0.3119596540927887, epoch: 80\n",
            "Train-->Loss: 0.3118444085121155, epoch: 90\n",
            "Test --> Loss: 0.3118444085121155, epoch: 90\n",
            "Train-->Loss: 0.31172922253608704, epoch: 100\n",
            "Test --> Loss: 0.31172922253608704, epoch: 100\n",
            "Train-->Loss: 0.3116139769554138, epoch: 110\n",
            "Test --> Loss: 0.3116139769554138, epoch: 110\n",
            "Train-->Loss: 0.3114987909793854, epoch: 120\n",
            "Test --> Loss: 0.3114987909793854, epoch: 120\n",
            "Train-->Loss: 0.31138360500335693, epoch: 130\n",
            "Test --> Loss: 0.31138360500335693, epoch: 130\n",
            "Train-->Loss: 0.3112683892250061, epoch: 140\n",
            "Test --> Loss: 0.3112683892250061, epoch: 140\n",
            "Train-->Loss: 0.3111531734466553, epoch: 150\n",
            "Test --> Loss: 0.3111531734466553, epoch: 150\n",
            "Train-->Loss: 0.31103792786598206, epoch: 160\n",
            "Test --> Loss: 0.31103792786598206, epoch: 160\n",
            "Train-->Loss: 0.3109227120876312, epoch: 170\n",
            "Test --> Loss: 0.3109227120876312, epoch: 170\n",
            "Train-->Loss: 0.3108075261116028, epoch: 180\n",
            "Test --> Loss: 0.3108075261116028, epoch: 180\n",
            "Train-->Loss: 0.31069228053092957, epoch: 190\n",
            "Test --> Loss: 0.31069228053092957, epoch: 190\n",
            "Train-->Loss: 0.3105770945549011, epoch: 200\n",
            "Test --> Loss: 0.3105770945549011, epoch: 200\n",
            "Train-->Loss: 0.3104618787765503, epoch: 210\n",
            "Test --> Loss: 0.3104618787765503, epoch: 210\n",
            "Train-->Loss: 0.31034666299819946, epoch: 220\n",
            "Test --> Loss: 0.31034666299819946, epoch: 220\n",
            "Train-->Loss: 0.310231477022171, epoch: 230\n",
            "Test --> Loss: 0.310231477022171, epoch: 230\n",
            "Train-->Loss: 0.3101162314414978, epoch: 240\n",
            "Test --> Loss: 0.3101162314414978, epoch: 240\n",
            "Train-->Loss: 0.31000104546546936, epoch: 250\n",
            "Test --> Loss: 0.31000104546546936, epoch: 250\n",
            "Train-->Loss: 0.30988579988479614, epoch: 260\n",
            "Test --> Loss: 0.30988579988479614, epoch: 260\n",
            "Train-->Loss: 0.3097706437110901, epoch: 270\n",
            "Test --> Loss: 0.3097706437110901, epoch: 270\n",
            "Train-->Loss: 0.30965539813041687, epoch: 280\n",
            "Test --> Loss: 0.30965539813041687, epoch: 280\n",
            "Train-->Loss: 0.3095402121543884, epoch: 290\n",
            "Test --> Loss: 0.3095402121543884, epoch: 290\n",
            "Train-->Loss: 0.3094249367713928, epoch: 300\n",
            "Test --> Loss: 0.3094249367713928, epoch: 300\n",
            "Train-->Loss: 0.3093097507953644, epoch: 310\n",
            "Test --> Loss: 0.3093097507953644, epoch: 310\n",
            "Train-->Loss: 0.30919456481933594, epoch: 320\n",
            "Test --> Loss: 0.30919456481933594, epoch: 320\n",
            "Train-->Loss: 0.3090793192386627, epoch: 330\n",
            "Test --> Loss: 0.3090793192386627, epoch: 330\n",
            "Train-->Loss: 0.3089641034603119, epoch: 340\n",
            "Test --> Loss: 0.3089641034603119, epoch: 340\n",
            "Train-->Loss: 0.30884891748428345, epoch: 350\n",
            "Test --> Loss: 0.30884891748428345, epoch: 350\n",
            "Train-->Loss: 0.30873367190361023, epoch: 360\n",
            "Test --> Loss: 0.30873367190361023, epoch: 360\n",
            "Train-->Loss: 0.3086184859275818, epoch: 370\n",
            "Test --> Loss: 0.3086184859275818, epoch: 370\n",
            "Train-->Loss: 0.30850327014923096, epoch: 380\n",
            "Test --> Loss: 0.30850327014923096, epoch: 380\n",
            "Train-->Loss: 0.3083880543708801, epoch: 390\n",
            "Test --> Loss: 0.3083880543708801, epoch: 390\n",
            "Train-->Loss: 0.3082728385925293, epoch: 400\n",
            "Test --> Loss: 0.3082728385925293, epoch: 400\n",
            "Train-->Loss: 0.30815762281417847, epoch: 410\n",
            "Test --> Loss: 0.30815762281417847, epoch: 410\n",
            "Train-->Loss: 0.30804243683815, epoch: 420\n",
            "Test --> Loss: 0.30804243683815, epoch: 420\n",
            "Train-->Loss: 0.3079272210597992, epoch: 430\n",
            "Test --> Loss: 0.3079272210597992, epoch: 430\n",
            "Train-->Loss: 0.307811975479126, epoch: 440\n",
            "Test --> Loss: 0.307811975479126, epoch: 440\n",
            "Train-->Loss: 0.30769675970077515, epoch: 450\n",
            "Test --> Loss: 0.30769675970077515, epoch: 450\n",
            "Train-->Loss: 0.3075815439224243, epoch: 460\n",
            "Test --> Loss: 0.3075815439224243, epoch: 460\n",
            "Train-->Loss: 0.3074663579463959, epoch: 470\n",
            "Test --> Loss: 0.3074663579463959, epoch: 470\n",
            "Train-->Loss: 0.30735111236572266, epoch: 480\n",
            "Test --> Loss: 0.30735111236572266, epoch: 480\n",
            "Train-->Loss: 0.3072359263896942, epoch: 490\n",
            "Test --> Loss: 0.3072359263896942, epoch: 490\n",
            "Train-->Loss: 0.30712074041366577, epoch: 500\n",
            "Test --> Loss: 0.30712074041366577, epoch: 500\n",
            "Train-->Loss: 0.30700546503067017, epoch: 510\n",
            "Test --> Loss: 0.30700546503067017, epoch: 510\n",
            "Train-->Loss: 0.3068902790546417, epoch: 520\n",
            "Test --> Loss: 0.3068902790546417, epoch: 520\n",
            "Train-->Loss: 0.3067750930786133, epoch: 530\n",
            "Test --> Loss: 0.3067750930786133, epoch: 530\n",
            "Train-->Loss: 0.30665984749794006, epoch: 540\n",
            "Test --> Loss: 0.30665984749794006, epoch: 540\n",
            "Train-->Loss: 0.3065446615219116, epoch: 550\n",
            "Test --> Loss: 0.3065446615219116, epoch: 550\n",
            "Train-->Loss: 0.306429386138916, epoch: 560\n",
            "Test --> Loss: 0.306429386138916, epoch: 560\n",
            "Train-->Loss: 0.30631422996520996, epoch: 570\n",
            "Test --> Loss: 0.30631422996520996, epoch: 570\n",
            "Train-->Loss: 0.3061990439891815, epoch: 580\n",
            "Test --> Loss: 0.3061990439891815, epoch: 580\n",
            "Train-->Loss: 0.3060837686061859, epoch: 590\n",
            "Test --> Loss: 0.3060837686061859, epoch: 590\n",
            "Train-->Loss: 0.30596858263015747, epoch: 600\n",
            "Test --> Loss: 0.30596858263015747, epoch: 600\n",
            "Train-->Loss: 0.30585336685180664, epoch: 610\n",
            "Test --> Loss: 0.30585336685180664, epoch: 610\n",
            "Train-->Loss: 0.3057381510734558, epoch: 620\n",
            "Test --> Loss: 0.3057381510734558, epoch: 620\n",
            "Train-->Loss: 0.30562296509742737, epoch: 630\n",
            "Test --> Loss: 0.30562296509742737, epoch: 630\n",
            "Train-->Loss: 0.30550774931907654, epoch: 640\n",
            "Test --> Loss: 0.30550774931907654, epoch: 640\n",
            "Train-->Loss: 0.3053925037384033, epoch: 650\n",
            "Test --> Loss: 0.3053925037384033, epoch: 650\n",
            "Train-->Loss: 0.3052772879600525, epoch: 660\n",
            "Test --> Loss: 0.3052772879600525, epoch: 660\n",
            "Train-->Loss: 0.30516210198402405, epoch: 670\n",
            "Test --> Loss: 0.30516210198402405, epoch: 670\n",
            "Train-->Loss: 0.3050468862056732, epoch: 680\n",
            "Test --> Loss: 0.3050468862056732, epoch: 680\n",
            "Train-->Loss: 0.3049316704273224, epoch: 690\n",
            "Test --> Loss: 0.3049316704273224, epoch: 690\n",
            "Train-->Loss: 0.30481642484664917, epoch: 700\n",
            "Test --> Loss: 0.30481642484664917, epoch: 700\n",
            "Train-->Loss: 0.3047012388706207, epoch: 710\n",
            "Test --> Loss: 0.3047012388706207, epoch: 710\n",
            "Train-->Loss: 0.3045860230922699, epoch: 720\n",
            "Test --> Loss: 0.3045860230922699, epoch: 720\n",
            "Train-->Loss: 0.30447080731391907, epoch: 730\n",
            "Test --> Loss: 0.30447080731391907, epoch: 730\n",
            "Train-->Loss: 0.3043556213378906, epoch: 740\n",
            "Test --> Loss: 0.3043556213378906, epoch: 740\n",
            "Train-->Loss: 0.3042404055595398, epoch: 750\n",
            "Test --> Loss: 0.3042404055595398, epoch: 750\n",
            "Train-->Loss: 0.3041251599788666, epoch: 760\n",
            "Test --> Loss: 0.3041251599788666, epoch: 760\n",
            "Train-->Loss: 0.30400997400283813, epoch: 770\n",
            "Test --> Loss: 0.30400997400283813, epoch: 770\n",
            "Train-->Loss: 0.3038947582244873, epoch: 780\n",
            "Test --> Loss: 0.3038947582244873, epoch: 780\n",
            "Train-->Loss: 0.3037795424461365, epoch: 790\n",
            "Test --> Loss: 0.3037795424461365, epoch: 790\n",
            "Train-->Loss: 0.30366429686546326, epoch: 800\n",
            "Test --> Loss: 0.30366429686546326, epoch: 800\n",
            "Train-->Loss: 0.3035491108894348, epoch: 810\n",
            "Test --> Loss: 0.3035491108894348, epoch: 810\n",
            "Train-->Loss: 0.30343392491340637, epoch: 820\n",
            "Test --> Loss: 0.30343392491340637, epoch: 820\n",
            "Train-->Loss: 0.30331867933273315, epoch: 830\n",
            "Test --> Loss: 0.30331867933273315, epoch: 830\n",
            "Train-->Loss: 0.3032034635543823, epoch: 840\n",
            "Test --> Loss: 0.3032034635543823, epoch: 840\n",
            "Train-->Loss: 0.3030882775783539, epoch: 850\n",
            "Test --> Loss: 0.3030882775783539, epoch: 850\n",
            "Train-->Loss: 0.30297306180000305, epoch: 860\n",
            "Test --> Loss: 0.30297306180000305, epoch: 860\n",
            "Train-->Loss: 0.3028578460216522, epoch: 870\n",
            "Test --> Loss: 0.3028578460216522, epoch: 870\n",
            "Train-->Loss: 0.3027426302433014, epoch: 880\n",
            "Test --> Loss: 0.3027426302433014, epoch: 880\n",
            "Train-->Loss: 0.30262741446495056, epoch: 890\n",
            "Test --> Loss: 0.30262741446495056, epoch: 890\n",
            "Train-->Loss: 0.30251219868659973, epoch: 900\n",
            "Test --> Loss: 0.30251219868659973, epoch: 900\n",
            "Train-->Loss: 0.3023969531059265, epoch: 910\n",
            "Test --> Loss: 0.3023969531059265, epoch: 910\n",
            "Train-->Loss: 0.30228179693222046, epoch: 920\n",
            "Test --> Loss: 0.30228179693222046, epoch: 920\n",
            "Train-->Loss: 0.30216658115386963, epoch: 930\n",
            "Test --> Loss: 0.30216658115386963, epoch: 930\n",
            "Train-->Loss: 0.3020513355731964, epoch: 940\n",
            "Test --> Loss: 0.3020513355731964, epoch: 940\n",
            "Train-->Loss: 0.3019361197948456, epoch: 950\n",
            "Test --> Loss: 0.3019361197948456, epoch: 950\n",
            "Train-->Loss: 0.30182090401649475, epoch: 960\n",
            "Test --> Loss: 0.30182090401649475, epoch: 960\n",
            "Train-->Loss: 0.3017056882381439, epoch: 970\n",
            "Test --> Loss: 0.3017056882381439, epoch: 970\n",
            "Train-->Loss: 0.3015905022621155, epoch: 980\n",
            "Test --> Loss: 0.3015905022621155, epoch: 980\n",
            "Train-->Loss: 0.30147525668144226, epoch: 990\n",
            "Test --> Loss: 0.30147525668144226, epoch: 990\n",
            "Train-->Loss: 0.3013600707054138, epoch: 1000\n",
            "Test --> Loss: 0.3013600707054138, epoch: 1000\n",
            "Train-->Loss: 0.301244854927063, epoch: 1010\n",
            "Test --> Loss: 0.301244854927063, epoch: 1010\n",
            "Train-->Loss: 0.30112963914871216, epoch: 1020\n",
            "Test --> Loss: 0.30112963914871216, epoch: 1020\n",
            "Train-->Loss: 0.3010144531726837, epoch: 1030\n",
            "Test --> Loss: 0.3010144531726837, epoch: 1030\n",
            "Train-->Loss: 0.3008992075920105, epoch: 1040\n",
            "Test --> Loss: 0.3008992075920105, epoch: 1040\n",
            "Train-->Loss: 0.30078399181365967, epoch: 1050\n",
            "Test --> Loss: 0.30078399181365967, epoch: 1050\n",
            "Train-->Loss: 0.3006688058376312, epoch: 1060\n",
            "Test --> Loss: 0.3006688058376312, epoch: 1060\n",
            "Train-->Loss: 0.3005536198616028, epoch: 1070\n",
            "Test --> Loss: 0.3005536198616028, epoch: 1070\n",
            "Train-->Loss: 0.30043840408325195, epoch: 1080\n",
            "Test --> Loss: 0.30043840408325195, epoch: 1080\n",
            "Train-->Loss: 0.30032315850257874, epoch: 1090\n",
            "Test --> Loss: 0.30032315850257874, epoch: 1090\n",
            "Train-->Loss: 0.3002079427242279, epoch: 1100\n",
            "Test --> Loss: 0.3002079427242279, epoch: 1100\n",
            "Train-->Loss: 0.3000927269458771, epoch: 1110\n",
            "Test --> Loss: 0.3000927269458771, epoch: 1110\n",
            "Train-->Loss: 0.29997754096984863, epoch: 1120\n",
            "Test --> Loss: 0.29997754096984863, epoch: 1120\n",
            "Train-->Loss: 0.2998623251914978, epoch: 1130\n",
            "Test --> Loss: 0.2998623251914978, epoch: 1130\n",
            "Train-->Loss: 0.299747109413147, epoch: 1140\n",
            "Test --> Loss: 0.299747109413147, epoch: 1140\n",
            "Train-->Loss: 0.29963186383247375, epoch: 1150\n",
            "Test --> Loss: 0.29963186383247375, epoch: 1150\n",
            "Train-->Loss: 0.2995166480541229, epoch: 1160\n",
            "Test --> Loss: 0.2995166480541229, epoch: 1160\n",
            "Train-->Loss: 0.2994014620780945, epoch: 1170\n",
            "Test --> Loss: 0.2994014620780945, epoch: 1170\n",
            "Train-->Loss: 0.29928624629974365, epoch: 1180\n",
            "Test --> Loss: 0.29928624629974365, epoch: 1180\n",
            "Train-->Loss: 0.2991710305213928, epoch: 1190\n",
            "Test --> Loss: 0.2991710305213928, epoch: 1190\n",
            "Train-->Loss: 0.299055814743042, epoch: 1200\n",
            "Test --> Loss: 0.299055814743042, epoch: 1200\n",
            "Train-->Loss: 0.29894059896469116, epoch: 1210\n",
            "Test --> Loss: 0.29894059896469116, epoch: 1210\n",
            "Train-->Loss: 0.2988254427909851, epoch: 1220\n",
            "Test --> Loss: 0.2988254427909851, epoch: 1220\n",
            "Train-->Loss: 0.2987101674079895, epoch: 1230\n",
            "Test --> Loss: 0.2987101674079895, epoch: 1230\n",
            "Train-->Loss: 0.29859498143196106, epoch: 1240\n",
            "Test --> Loss: 0.29859498143196106, epoch: 1240\n",
            "Train-->Loss: 0.29847976565361023, epoch: 1250\n",
            "Test --> Loss: 0.29847976565361023, epoch: 1250\n",
            "Train-->Loss: 0.298364520072937, epoch: 1260\n",
            "Test --> Loss: 0.298364520072937, epoch: 1260\n",
            "Train-->Loss: 0.2982493042945862, epoch: 1270\n",
            "Test --> Loss: 0.2982493042945862, epoch: 1270\n",
            "Train-->Loss: 0.2981341481208801, epoch: 1280\n",
            "Test --> Loss: 0.2981341481208801, epoch: 1280\n",
            "Train-->Loss: 0.2980188727378845, epoch: 1290\n",
            "Test --> Loss: 0.2980188727378845, epoch: 1290\n",
            "Train-->Loss: 0.2979036867618561, epoch: 1300\n",
            "Test --> Loss: 0.2979036867618561, epoch: 1300\n",
            "Train-->Loss: 0.29778844118118286, epoch: 1310\n",
            "Test --> Loss: 0.29778844118118286, epoch: 1310\n",
            "Train-->Loss: 0.2976732850074768, epoch: 1320\n",
            "Test --> Loss: 0.2976732850074768, epoch: 1320\n",
            "Train-->Loss: 0.29755809903144836, epoch: 1330\n",
            "Test --> Loss: 0.29755809903144836, epoch: 1330\n",
            "Train-->Loss: 0.29744285345077515, epoch: 1340\n",
            "Test --> Loss: 0.29744285345077515, epoch: 1340\n",
            "Train-->Loss: 0.2973276376724243, epoch: 1350\n",
            "Test --> Loss: 0.2973276376724243, epoch: 1350\n",
            "Train-->Loss: 0.2972124218940735, epoch: 1360\n",
            "Test --> Loss: 0.2972124218940735, epoch: 1360\n",
            "Train-->Loss: 0.29709717631340027, epoch: 1370\n",
            "Test --> Loss: 0.29709717631340027, epoch: 1370\n",
            "Train-->Loss: 0.2969819903373718, epoch: 1380\n",
            "Test --> Loss: 0.2969819903373718, epoch: 1380\n",
            "Train-->Loss: 0.296866774559021, epoch: 1390\n",
            "Test --> Loss: 0.296866774559021, epoch: 1390\n",
            "Train-->Loss: 0.29675155878067017, epoch: 1400\n",
            "Test --> Loss: 0.29675155878067017, epoch: 1400\n",
            "Train-->Loss: 0.29663634300231934, epoch: 1410\n",
            "Test --> Loss: 0.29663634300231934, epoch: 1410\n",
            "Train-->Loss: 0.2965211272239685, epoch: 1420\n",
            "Test --> Loss: 0.2965211272239685, epoch: 1420\n",
            "Train-->Loss: 0.29640594124794006, epoch: 1430\n",
            "Test --> Loss: 0.29640594124794006, epoch: 1430\n",
            "Train-->Loss: 0.29629069566726685, epoch: 1440\n",
            "Test --> Loss: 0.29629069566726685, epoch: 1440\n",
            "Train-->Loss: 0.2961755096912384, epoch: 1450\n",
            "Test --> Loss: 0.2961755096912384, epoch: 1450\n",
            "Train-->Loss: 0.2960602641105652, epoch: 1460\n",
            "Test --> Loss: 0.2960602641105652, epoch: 1460\n",
            "Train-->Loss: 0.29594507813453674, epoch: 1470\n",
            "Test --> Loss: 0.29594507813453674, epoch: 1470\n",
            "Train-->Loss: 0.2958298623561859, epoch: 1480\n",
            "Test --> Loss: 0.2958298623561859, epoch: 1480\n",
            "Train-->Loss: 0.29571467638015747, epoch: 1490\n",
            "Test --> Loss: 0.29571467638015747, epoch: 1490\n",
            "Train-->Loss: 0.29559940099716187, epoch: 1500\n",
            "Test --> Loss: 0.29559940099716187, epoch: 1500\n",
            "Train-->Loss: 0.2954842150211334, epoch: 1510\n",
            "Test --> Loss: 0.2954842150211334, epoch: 1510\n",
            "Train-->Loss: 0.2953689992427826, epoch: 1520\n",
            "Test --> Loss: 0.2953689992427826, epoch: 1520\n",
            "Train-->Loss: 0.29525381326675415, epoch: 1530\n",
            "Test --> Loss: 0.29525381326675415, epoch: 1530\n",
            "Train-->Loss: 0.29513856768608093, epoch: 1540\n",
            "Test --> Loss: 0.29513856768608093, epoch: 1540\n",
            "Train-->Loss: 0.2950233519077301, epoch: 1550\n",
            "Test --> Loss: 0.2950233519077301, epoch: 1550\n",
            "Train-->Loss: 0.29490816593170166, epoch: 1560\n",
            "Test --> Loss: 0.29490816593170166, epoch: 1560\n",
            "Train-->Loss: 0.29479295015335083, epoch: 1570\n",
            "Test --> Loss: 0.29479295015335083, epoch: 1570\n",
            "Train-->Loss: 0.2946777045726776, epoch: 1580\n",
            "Test --> Loss: 0.2946777045726776, epoch: 1580\n",
            "Train-->Loss: 0.29456251859664917, epoch: 1590\n",
            "Test --> Loss: 0.29456251859664917, epoch: 1590\n",
            "Train-->Loss: 0.2944473326206207, epoch: 1600\n",
            "Test --> Loss: 0.2944473326206207, epoch: 1600\n",
            "Train-->Loss: 0.2943320870399475, epoch: 1610\n",
            "Test --> Loss: 0.2943320870399475, epoch: 1610\n",
            "Train-->Loss: 0.29421690106391907, epoch: 1620\n",
            "Test --> Loss: 0.29421690106391907, epoch: 1620\n",
            "Train-->Loss: 0.29410165548324585, epoch: 1630\n",
            "Test --> Loss: 0.29410165548324585, epoch: 1630\n",
            "Train-->Loss: 0.2939864695072174, epoch: 1640\n",
            "Test --> Loss: 0.2939864695072174, epoch: 1640\n",
            "Train-->Loss: 0.29387128353118896, epoch: 1650\n",
            "Test --> Loss: 0.29387128353118896, epoch: 1650\n",
            "Train-->Loss: 0.29375603795051575, epoch: 1660\n",
            "Test --> Loss: 0.29375603795051575, epoch: 1660\n",
            "Train-->Loss: 0.2936408221721649, epoch: 1670\n",
            "Test --> Loss: 0.2936408221721649, epoch: 1670\n",
            "Train-->Loss: 0.2935256063938141, epoch: 1680\n",
            "Test --> Loss: 0.2935256063938141, epoch: 1680\n",
            "Train-->Loss: 0.29341036081314087, epoch: 1690\n",
            "Test --> Loss: 0.29341036081314087, epoch: 1690\n",
            "Train-->Loss: 0.2932951748371124, epoch: 1700\n",
            "Test --> Loss: 0.2932951748371124, epoch: 1700\n",
            "Train-->Loss: 0.2931799292564392, epoch: 1710\n",
            "Test --> Loss: 0.2931799292564392, epoch: 1710\n",
            "Train-->Loss: 0.29306474328041077, epoch: 1720\n",
            "Test --> Loss: 0.29306474328041077, epoch: 1720\n",
            "Train-->Loss: 0.29294952750205994, epoch: 1730\n",
            "Test --> Loss: 0.29294952750205994, epoch: 1730\n",
            "Train-->Loss: 0.2928343415260315, epoch: 1740\n",
            "Test --> Loss: 0.2928343415260315, epoch: 1740\n",
            "Train-->Loss: 0.29271912574768066, epoch: 1750\n",
            "Test --> Loss: 0.29271912574768066, epoch: 1750\n",
            "Train-->Loss: 0.29260388016700745, epoch: 1760\n",
            "Test --> Loss: 0.29260388016700745, epoch: 1760\n",
            "Train-->Loss: 0.2924886643886566, epoch: 1770\n",
            "Test --> Loss: 0.2924886643886566, epoch: 1770\n",
            "Train-->Loss: 0.2923734784126282, epoch: 1780\n",
            "Test --> Loss: 0.2923734784126282, epoch: 1780\n",
            "Train-->Loss: 0.29225823283195496, epoch: 1790\n",
            "Test --> Loss: 0.29225823283195496, epoch: 1790\n",
            "Train-->Loss: 0.2921430468559265, epoch: 1800\n",
            "Test --> Loss: 0.2921430468559265, epoch: 1800\n",
            "Train-->Loss: 0.2920278310775757, epoch: 1810\n",
            "Test --> Loss: 0.2920278310775757, epoch: 1810\n",
            "Train-->Loss: 0.29191261529922485, epoch: 1820\n",
            "Test --> Loss: 0.29191261529922485, epoch: 1820\n",
            "Train-->Loss: 0.2917974293231964, epoch: 1830\n",
            "Test --> Loss: 0.2917974293231964, epoch: 1830\n",
            "Train-->Loss: 0.2916821837425232, epoch: 1840\n",
            "Test --> Loss: 0.2916821837425232, epoch: 1840\n",
            "Train-->Loss: 0.29156699776649475, epoch: 1850\n",
            "Test --> Loss: 0.29156699776649475, epoch: 1850\n",
            "Train-->Loss: 0.29145175218582153, epoch: 1860\n",
            "Test --> Loss: 0.29145175218582153, epoch: 1860\n",
            "Train-->Loss: 0.2913365364074707, epoch: 1870\n",
            "Test --> Loss: 0.2913365364074707, epoch: 1870\n",
            "Train-->Loss: 0.29122135043144226, epoch: 1880\n",
            "Test --> Loss: 0.29122135043144226, epoch: 1880\n",
            "Train-->Loss: 0.29110613465309143, epoch: 1890\n",
            "Test --> Loss: 0.29110613465309143, epoch: 1890\n",
            "Train-->Loss: 0.2909909188747406, epoch: 1900\n",
            "Test --> Loss: 0.2909909188747406, epoch: 1900\n",
            "Train-->Loss: 0.29087570309638977, epoch: 1910\n",
            "Test --> Loss: 0.29087570309638977, epoch: 1910\n",
            "Train-->Loss: 0.29076051712036133, epoch: 1920\n",
            "Test --> Loss: 0.29076051712036133, epoch: 1920\n",
            "Train-->Loss: 0.2906452715396881, epoch: 1930\n",
            "Test --> Loss: 0.2906452715396881, epoch: 1930\n",
            "Train-->Loss: 0.2905300557613373, epoch: 1940\n",
            "Test --> Loss: 0.2905300557613373, epoch: 1940\n",
            "Train-->Loss: 0.29041483998298645, epoch: 1950\n",
            "Test --> Loss: 0.29041483998298645, epoch: 1950\n",
            "Train-->Loss: 0.2902996242046356, epoch: 1960\n",
            "Test --> Loss: 0.2902996242046356, epoch: 1960\n",
            "Train-->Loss: 0.2901844382286072, epoch: 1970\n",
            "Test --> Loss: 0.2901844382286072, epoch: 1970\n",
            "Train-->Loss: 0.29006922245025635, epoch: 1980\n",
            "Test --> Loss: 0.29006922245025635, epoch: 1980\n",
            "Train-->Loss: 0.2899540066719055, epoch: 1990\n",
            "Test --> Loss: 0.2899540066719055, epoch: 1990\n",
            "Train-->Loss: 0.2898387908935547, epoch: 2000\n",
            "Test --> Loss: 0.2898387908935547, epoch: 2000\n",
            "Train-->Loss: 0.28972357511520386, epoch: 2010\n",
            "Test --> Loss: 0.28972357511520386, epoch: 2010\n",
            "Train-->Loss: 0.2896083891391754, epoch: 2020\n",
            "Test --> Loss: 0.2896083891391754, epoch: 2020\n",
            "Train-->Loss: 0.2894931733608246, epoch: 2030\n",
            "Test --> Loss: 0.2894931733608246, epoch: 2030\n",
            "Train-->Loss: 0.28937792778015137, epoch: 2040\n",
            "Test --> Loss: 0.28937792778015137, epoch: 2040\n",
            "Train-->Loss: 0.28926271200180054, epoch: 2050\n",
            "Test --> Loss: 0.28926271200180054, epoch: 2050\n",
            "Train-->Loss: 0.2891475260257721, epoch: 2060\n",
            "Test --> Loss: 0.2891475260257721, epoch: 2060\n",
            "Train-->Loss: 0.28903231024742126, epoch: 2070\n",
            "Test --> Loss: 0.28903231024742126, epoch: 2070\n",
            "Train-->Loss: 0.28891706466674805, epoch: 2080\n",
            "Test --> Loss: 0.28891706466674805, epoch: 2080\n",
            "Train-->Loss: 0.2888018786907196, epoch: 2090\n",
            "Test --> Loss: 0.2888018786907196, epoch: 2090\n",
            "Train-->Loss: 0.28868669271469116, epoch: 2100\n",
            "Test --> Loss: 0.28868669271469116, epoch: 2100\n",
            "Train-->Loss: 0.28857141733169556, epoch: 2110\n",
            "Test --> Loss: 0.28857141733169556, epoch: 2110\n",
            "Train-->Loss: 0.2884562313556671, epoch: 2120\n",
            "Test --> Loss: 0.2884562313556671, epoch: 2120\n",
            "Train-->Loss: 0.28834104537963867, epoch: 2130\n",
            "Test --> Loss: 0.28834104537963867, epoch: 2130\n",
            "Train-->Loss: 0.28822579979896545, epoch: 2140\n",
            "Test --> Loss: 0.28822579979896545, epoch: 2140\n",
            "Train-->Loss: 0.288110613822937, epoch: 2150\n",
            "Test --> Loss: 0.288110613822937, epoch: 2150\n",
            "Train-->Loss: 0.2879953980445862, epoch: 2160\n",
            "Test --> Loss: 0.2879953980445862, epoch: 2160\n",
            "Train-->Loss: 0.28788018226623535, epoch: 2170\n",
            "Test --> Loss: 0.28788018226623535, epoch: 2170\n",
            "Train-->Loss: 0.2877649962902069, epoch: 2180\n",
            "Test --> Loss: 0.2877649962902069, epoch: 2180\n",
            "Train-->Loss: 0.2876497209072113, epoch: 2190\n",
            "Test --> Loss: 0.2876497209072113, epoch: 2190\n",
            "Train-->Loss: 0.28753453493118286, epoch: 2200\n",
            "Test --> Loss: 0.28753453493118286, epoch: 2200\n",
            "Train-->Loss: 0.28741931915283203, epoch: 2210\n",
            "Test --> Loss: 0.28741931915283203, epoch: 2210\n",
            "Train-->Loss: 0.2873041033744812, epoch: 2220\n",
            "Test --> Loss: 0.2873041033744812, epoch: 2220\n",
            "Train-->Loss: 0.28718891739845276, epoch: 2230\n",
            "Test --> Loss: 0.28718891739845276, epoch: 2230\n",
            "Train-->Loss: 0.28707370162010193, epoch: 2240\n",
            "Test --> Loss: 0.28707370162010193, epoch: 2240\n",
            "Train-->Loss: 0.2869584560394287, epoch: 2250\n",
            "Test --> Loss: 0.2869584560394287, epoch: 2250\n",
            "Train-->Loss: 0.2868432402610779, epoch: 2260\n",
            "Test --> Loss: 0.2868432402610779, epoch: 2260\n",
            "Train-->Loss: 0.28672805428504944, epoch: 2270\n",
            "Test --> Loss: 0.28672805428504944, epoch: 2270\n",
            "Train-->Loss: 0.2866128385066986, epoch: 2280\n",
            "Test --> Loss: 0.2866128385066986, epoch: 2280\n",
            "Train-->Loss: 0.2864976227283478, epoch: 2290\n",
            "Test --> Loss: 0.2864976227283478, epoch: 2290\n",
            "Train-->Loss: 0.28638237714767456, epoch: 2300\n",
            "Test --> Loss: 0.28638237714767456, epoch: 2300\n",
            "Train-->Loss: 0.2862671911716461, epoch: 2310\n",
            "Test --> Loss: 0.2862671911716461, epoch: 2310\n",
            "Train-->Loss: 0.2861519753932953, epoch: 2320\n",
            "Test --> Loss: 0.2861519753932953, epoch: 2320\n",
            "Train-->Loss: 0.28603675961494446, epoch: 2330\n",
            "Test --> Loss: 0.28603675961494446, epoch: 2330\n",
            "Train-->Loss: 0.285921573638916, epoch: 2340\n",
            "Test --> Loss: 0.285921573638916, epoch: 2340\n",
            "Train-->Loss: 0.2858063578605652, epoch: 2350\n",
            "Test --> Loss: 0.2858063578605652, epoch: 2350\n",
            "Train-->Loss: 0.28569111227989197, epoch: 2360\n",
            "Test --> Loss: 0.28569111227989197, epoch: 2360\n",
            "Train-->Loss: 0.2855759263038635, epoch: 2370\n",
            "Test --> Loss: 0.2855759263038635, epoch: 2370\n",
            "Train-->Loss: 0.2854607105255127, epoch: 2380\n",
            "Test --> Loss: 0.2854607105255127, epoch: 2380\n",
            "Train-->Loss: 0.28534549474716187, epoch: 2390\n",
            "Test --> Loss: 0.28534549474716187, epoch: 2390\n",
            "Train-->Loss: 0.28523024916648865, epoch: 2400\n",
            "Test --> Loss: 0.28523024916648865, epoch: 2400\n",
            "Train-->Loss: 0.2851150631904602, epoch: 2410\n",
            "Test --> Loss: 0.2851150631904602, epoch: 2410\n",
            "Train-->Loss: 0.28499987721443176, epoch: 2420\n",
            "Test --> Loss: 0.28499987721443176, epoch: 2420\n",
            "Train-->Loss: 0.28488463163375854, epoch: 2430\n",
            "Test --> Loss: 0.28488463163375854, epoch: 2430\n",
            "Train-->Loss: 0.2847694158554077, epoch: 2440\n",
            "Test --> Loss: 0.2847694158554077, epoch: 2440\n",
            "Train-->Loss: 0.2846542298793793, epoch: 2450\n",
            "Test --> Loss: 0.2846542298793793, epoch: 2450\n",
            "Train-->Loss: 0.28453901410102844, epoch: 2460\n",
            "Test --> Loss: 0.28453901410102844, epoch: 2460\n",
            "Train-->Loss: 0.2844237983226776, epoch: 2470\n",
            "Test --> Loss: 0.2844237983226776, epoch: 2470\n",
            "Train-->Loss: 0.2843085825443268, epoch: 2480\n",
            "Test --> Loss: 0.2843085825443268, epoch: 2480\n",
            "Train-->Loss: 0.28419336676597595, epoch: 2490\n",
            "Test --> Loss: 0.28419336676597595, epoch: 2490\n",
            "Train-->Loss: 0.2840781509876251, epoch: 2500\n",
            "Test --> Loss: 0.2840781509876251, epoch: 2500\n",
            "Train-->Loss: 0.2839629054069519, epoch: 2510\n",
            "Test --> Loss: 0.2839629054069519, epoch: 2510\n",
            "Train-->Loss: 0.28384774923324585, epoch: 2520\n",
            "Test --> Loss: 0.28384774923324585, epoch: 2520\n",
            "Train-->Loss: 0.283732533454895, epoch: 2530\n",
            "Test --> Loss: 0.283732533454895, epoch: 2530\n",
            "Train-->Loss: 0.2836172878742218, epoch: 2540\n",
            "Test --> Loss: 0.2836172878742218, epoch: 2540\n",
            "Train-->Loss: 0.28350207209587097, epoch: 2550\n",
            "Test --> Loss: 0.28350207209587097, epoch: 2550\n",
            "Train-->Loss: 0.28338685631752014, epoch: 2560\n",
            "Test --> Loss: 0.28338685631752014, epoch: 2560\n",
            "Train-->Loss: 0.2832716405391693, epoch: 2570\n",
            "Test --> Loss: 0.2832716405391693, epoch: 2570\n",
            "Train-->Loss: 0.28315645456314087, epoch: 2580\n",
            "Test --> Loss: 0.28315645456314087, epoch: 2580\n",
            "Train-->Loss: 0.28304120898246765, epoch: 2590\n",
            "Test --> Loss: 0.28304120898246765, epoch: 2590\n",
            "Train-->Loss: 0.2829260230064392, epoch: 2600\n",
            "Test --> Loss: 0.2829260230064392, epoch: 2600\n",
            "Train-->Loss: 0.2828108072280884, epoch: 2610\n",
            "Test --> Loss: 0.2828108072280884, epoch: 2610\n",
            "Train-->Loss: 0.28269559144973755, epoch: 2620\n",
            "Test --> Loss: 0.28269559144973755, epoch: 2620\n",
            "Train-->Loss: 0.2825804054737091, epoch: 2630\n",
            "Test --> Loss: 0.2825804054737091, epoch: 2630\n",
            "Train-->Loss: 0.2824651598930359, epoch: 2640\n",
            "Test --> Loss: 0.2824651598930359, epoch: 2640\n",
            "Train-->Loss: 0.28234994411468506, epoch: 2650\n",
            "Test --> Loss: 0.28234994411468506, epoch: 2650\n",
            "Train-->Loss: 0.2822347581386566, epoch: 2660\n",
            "Test --> Loss: 0.2822347581386566, epoch: 2660\n",
            "Train-->Loss: 0.2821195721626282, epoch: 2670\n",
            "Test --> Loss: 0.2821195721626282, epoch: 2670\n",
            "Train-->Loss: 0.28200435638427734, epoch: 2680\n",
            "Test --> Loss: 0.28200435638427734, epoch: 2680\n",
            "Train-->Loss: 0.2818891406059265, epoch: 2690\n",
            "Test --> Loss: 0.2818891406059265, epoch: 2690\n",
            "Train-->Loss: 0.2817738950252533, epoch: 2700\n",
            "Test --> Loss: 0.2817738950252533, epoch: 2700\n",
            "Train-->Loss: 0.28165867924690247, epoch: 2710\n",
            "Test --> Loss: 0.28165867924690247, epoch: 2710\n",
            "Train-->Loss: 0.28154343366622925, epoch: 2720\n",
            "Test --> Loss: 0.28154343366622925, epoch: 2720\n",
            "Train-->Loss: 0.2814282774925232, epoch: 2730\n",
            "Test --> Loss: 0.2814282774925232, epoch: 2730\n",
            "Train-->Loss: 0.28131306171417236, epoch: 2740\n",
            "Test --> Loss: 0.28131306171417236, epoch: 2740\n",
            "Train-->Loss: 0.28119781613349915, epoch: 2750\n",
            "Test --> Loss: 0.28119781613349915, epoch: 2750\n",
            "Train-->Loss: 0.2810826003551483, epoch: 2760\n",
            "Test --> Loss: 0.2810826003551483, epoch: 2760\n",
            "Train-->Loss: 0.2809674143791199, epoch: 2770\n",
            "Test --> Loss: 0.2809674143791199, epoch: 2770\n",
            "Train-->Loss: 0.28085219860076904, epoch: 2780\n",
            "Test --> Loss: 0.28085219860076904, epoch: 2780\n",
            "Train-->Loss: 0.2807369828224182, epoch: 2790\n",
            "Test --> Loss: 0.2807369828224182, epoch: 2790\n",
            "Train-->Loss: 0.2806217670440674, epoch: 2800\n",
            "Test --> Loss: 0.2806217670440674, epoch: 2800\n",
            "Train-->Loss: 0.28050655126571655, epoch: 2810\n",
            "Test --> Loss: 0.28050655126571655, epoch: 2810\n",
            "Train-->Loss: 0.2803913354873657, epoch: 2820\n",
            "Test --> Loss: 0.2803913354873657, epoch: 2820\n",
            "Train-->Loss: 0.2802761495113373, epoch: 2830\n",
            "Test --> Loss: 0.2802761495113373, epoch: 2830\n",
            "Train-->Loss: 0.28016090393066406, epoch: 2840\n",
            "Test --> Loss: 0.28016090393066406, epoch: 2840\n",
            "Train-->Loss: 0.2800457179546356, epoch: 2850\n",
            "Test --> Loss: 0.2800457179546356, epoch: 2850\n",
            "Train-->Loss: 0.2799304723739624, epoch: 2860\n",
            "Test --> Loss: 0.2799304723739624, epoch: 2860\n",
            "Train-->Loss: 0.27981528639793396, epoch: 2870\n",
            "Test --> Loss: 0.27981528639793396, epoch: 2870\n",
            "Train-->Loss: 0.27970004081726074, epoch: 2880\n",
            "Test --> Loss: 0.27970004081726074, epoch: 2880\n",
            "Train-->Loss: 0.2795848250389099, epoch: 2890\n",
            "Test --> Loss: 0.2795848250389099, epoch: 2890\n",
            "Train-->Loss: 0.27946963906288147, epoch: 2900\n",
            "Test --> Loss: 0.27946963906288147, epoch: 2900\n",
            "Train-->Loss: 0.27935442328453064, epoch: 2910\n",
            "Test --> Loss: 0.27935442328453064, epoch: 2910\n",
            "Train-->Loss: 0.2792392373085022, epoch: 2920\n",
            "Test --> Loss: 0.2792392373085022, epoch: 2920\n",
            "Train-->Loss: 0.279123991727829, epoch: 2930\n",
            "Test --> Loss: 0.279123991727829, epoch: 2930\n",
            "Train-->Loss: 0.27900880575180054, epoch: 2940\n",
            "Test --> Loss: 0.27900880575180054, epoch: 2940\n",
            "Train-->Loss: 0.2788935899734497, epoch: 2950\n",
            "Test --> Loss: 0.2788935899734497, epoch: 2950\n",
            "Train-->Loss: 0.2787783741950989, epoch: 2960\n",
            "Test --> Loss: 0.2787783741950989, epoch: 2960\n",
            "Train-->Loss: 0.27866312861442566, epoch: 2970\n",
            "Test --> Loss: 0.27866312861442566, epoch: 2970\n",
            "Train-->Loss: 0.2785479426383972, epoch: 2980\n",
            "Test --> Loss: 0.2785479426383972, epoch: 2980\n",
            "Train-->Loss: 0.2784327268600464, epoch: 2990\n",
            "Test --> Loss: 0.2784327268600464, epoch: 2990\n",
            "Train-->Loss: 0.27831751108169556, epoch: 3000\n",
            "Test --> Loss: 0.27831751108169556, epoch: 3000\n",
            "Train-->Loss: 0.2782022953033447, epoch: 3010\n",
            "Test --> Loss: 0.2782022953033447, epoch: 3010\n",
            "Train-->Loss: 0.2780870795249939, epoch: 3020\n",
            "Test --> Loss: 0.2780870795249939, epoch: 3020\n",
            "Train-->Loss: 0.27797189354896545, epoch: 3030\n",
            "Test --> Loss: 0.27797189354896545, epoch: 3030\n",
            "Train-->Loss: 0.27785664796829224, epoch: 3040\n",
            "Test --> Loss: 0.27785664796829224, epoch: 3040\n",
            "Train-->Loss: 0.2777414619922638, epoch: 3050\n",
            "Test --> Loss: 0.2777414619922638, epoch: 3050\n",
            "Train-->Loss: 0.2776262164115906, epoch: 3060\n",
            "Test --> Loss: 0.2776262164115906, epoch: 3060\n",
            "Train-->Loss: 0.27751103043556213, epoch: 3070\n",
            "Test --> Loss: 0.27751103043556213, epoch: 3070\n",
            "Train-->Loss: 0.2773957848548889, epoch: 3080\n",
            "Test --> Loss: 0.2773957848548889, epoch: 3080\n",
            "Train-->Loss: 0.2772805988788605, epoch: 3090\n",
            "Test --> Loss: 0.2772805988788605, epoch: 3090\n",
            "Train-->Loss: 0.27716535329818726, epoch: 3100\n",
            "Test --> Loss: 0.27716535329818726, epoch: 3100\n",
            "Train-->Loss: 0.2770501673221588, epoch: 3110\n",
            "Test --> Loss: 0.2770501673221588, epoch: 3110\n",
            "Train-->Loss: 0.276934951543808, epoch: 3120\n",
            "Test --> Loss: 0.276934951543808, epoch: 3120\n",
            "Train-->Loss: 0.27681976556777954, epoch: 3130\n",
            "Test --> Loss: 0.27681976556777954, epoch: 3130\n",
            "Train-->Loss: 0.2767045199871063, epoch: 3140\n",
            "Test --> Loss: 0.2767045199871063, epoch: 3140\n",
            "Train-->Loss: 0.2765893042087555, epoch: 3150\n",
            "Test --> Loss: 0.2765893042087555, epoch: 3150\n",
            "Train-->Loss: 0.27647411823272705, epoch: 3160\n",
            "Test --> Loss: 0.27647411823272705, epoch: 3160\n",
            "Train-->Loss: 0.2763589024543762, epoch: 3170\n",
            "Test --> Loss: 0.2763589024543762, epoch: 3170\n",
            "Train-->Loss: 0.2762437164783478, epoch: 3180\n",
            "Test --> Loss: 0.2762437164783478, epoch: 3180\n",
            "Train-->Loss: 0.27612847089767456, epoch: 3190\n",
            "Test --> Loss: 0.27612847089767456, epoch: 3190\n",
            "Train-->Loss: 0.27601322531700134, epoch: 3200\n",
            "Test --> Loss: 0.27601322531700134, epoch: 3200\n",
            "Train-->Loss: 0.2758980393409729, epoch: 3210\n",
            "Test --> Loss: 0.2758980393409729, epoch: 3210\n",
            "Train-->Loss: 0.27578285336494446, epoch: 3220\n",
            "Test --> Loss: 0.27578285336494446, epoch: 3220\n",
            "Train-->Loss: 0.27566760778427124, epoch: 3230\n",
            "Test --> Loss: 0.27566760778427124, epoch: 3230\n",
            "Train-->Loss: 0.2755524218082428, epoch: 3240\n",
            "Test --> Loss: 0.2755524218082428, epoch: 3240\n",
            "Train-->Loss: 0.2754371762275696, epoch: 3250\n",
            "Test --> Loss: 0.2754371762275696, epoch: 3250\n",
            "Train-->Loss: 0.27532199025154114, epoch: 3260\n",
            "Test --> Loss: 0.27532199025154114, epoch: 3260\n",
            "Train-->Loss: 0.2752067744731903, epoch: 3270\n",
            "Test --> Loss: 0.2752067744731903, epoch: 3270\n",
            "Train-->Loss: 0.2750915586948395, epoch: 3280\n",
            "Test --> Loss: 0.2750915586948395, epoch: 3280\n",
            "Train-->Loss: 0.27497631311416626, epoch: 3290\n",
            "Test --> Loss: 0.27497631311416626, epoch: 3290\n",
            "Train-->Loss: 0.2748611271381378, epoch: 3300\n",
            "Test --> Loss: 0.2748611271381378, epoch: 3300\n",
            "Train-->Loss: 0.2747458815574646, epoch: 3310\n",
            "Test --> Loss: 0.2747458815574646, epoch: 3310\n",
            "Train-->Loss: 0.27463069558143616, epoch: 3320\n",
            "Test --> Loss: 0.27463069558143616, epoch: 3320\n",
            "Train-->Loss: 0.2745154798030853, epoch: 3330\n",
            "Test --> Loss: 0.2745154798030853, epoch: 3330\n",
            "Train-->Loss: 0.2744002938270569, epoch: 3340\n",
            "Test --> Loss: 0.2744002938270569, epoch: 3340\n",
            "Train-->Loss: 0.27428507804870605, epoch: 3350\n",
            "Test --> Loss: 0.27428507804870605, epoch: 3350\n",
            "Train-->Loss: 0.27416983246803284, epoch: 3360\n",
            "Test --> Loss: 0.27416983246803284, epoch: 3360\n",
            "Train-->Loss: 0.274054616689682, epoch: 3370\n",
            "Test --> Loss: 0.274054616689682, epoch: 3370\n",
            "Train-->Loss: 0.27393943071365356, epoch: 3380\n",
            "Test --> Loss: 0.27393943071365356, epoch: 3380\n",
            "Train-->Loss: 0.27382418513298035, epoch: 3390\n",
            "Test --> Loss: 0.27382418513298035, epoch: 3390\n",
            "Train-->Loss: 0.2737089991569519, epoch: 3400\n",
            "Test --> Loss: 0.2737089991569519, epoch: 3400\n",
            "Train-->Loss: 0.2735937833786011, epoch: 3410\n",
            "Test --> Loss: 0.2735937833786011, epoch: 3410\n",
            "Train-->Loss: 0.27347856760025024, epoch: 3420\n",
            "Test --> Loss: 0.27347856760025024, epoch: 3420\n",
            "Train-->Loss: 0.2733633816242218, epoch: 3430\n",
            "Test --> Loss: 0.2733633816242218, epoch: 3430\n",
            "Train-->Loss: 0.2732481360435486, epoch: 3440\n",
            "Test --> Loss: 0.2732481360435486, epoch: 3440\n",
            "Train-->Loss: 0.27313295006752014, epoch: 3450\n",
            "Test --> Loss: 0.27313295006752014, epoch: 3450\n",
            "Train-->Loss: 0.2730177044868469, epoch: 3460\n",
            "Test --> Loss: 0.2730177044868469, epoch: 3460\n",
            "Train-->Loss: 0.27290254831314087, epoch: 3470\n",
            "Test --> Loss: 0.27290254831314087, epoch: 3470\n",
            "Train-->Loss: 0.27278730273246765, epoch: 3480\n",
            "Test --> Loss: 0.27278730273246765, epoch: 3480\n",
            "Train-->Loss: 0.2726721167564392, epoch: 3490\n",
            "Test --> Loss: 0.2726721167564392, epoch: 3490\n",
            "Train-->Loss: 0.2725568413734436, epoch: 3500\n",
            "Test --> Loss: 0.2725568413734436, epoch: 3500\n",
            "Train-->Loss: 0.27244165539741516, epoch: 3510\n",
            "Test --> Loss: 0.27244165539741516, epoch: 3510\n",
            "Train-->Loss: 0.27232640981674194, epoch: 3520\n",
            "Test --> Loss: 0.27232640981674194, epoch: 3520\n",
            "Train-->Loss: 0.2722112238407135, epoch: 3530\n",
            "Test --> Loss: 0.2722112238407135, epoch: 3530\n",
            "Train-->Loss: 0.27209600806236267, epoch: 3540\n",
            "Test --> Loss: 0.27209600806236267, epoch: 3540\n",
            "Train-->Loss: 0.27198079228401184, epoch: 3550\n",
            "Test --> Loss: 0.27198079228401184, epoch: 3550\n",
            "Train-->Loss: 0.271865576505661, epoch: 3560\n",
            "Test --> Loss: 0.271865576505661, epoch: 3560\n",
            "Train-->Loss: 0.27175039052963257, epoch: 3570\n",
            "Test --> Loss: 0.27175039052963257, epoch: 3570\n",
            "Train-->Loss: 0.27163517475128174, epoch: 3580\n",
            "Test --> Loss: 0.27163517475128174, epoch: 3580\n",
            "Train-->Loss: 0.2715199589729309, epoch: 3590\n",
            "Test --> Loss: 0.2715199589729309, epoch: 3590\n",
            "Train-->Loss: 0.2714047431945801, epoch: 3600\n",
            "Test --> Loss: 0.2714047431945801, epoch: 3600\n",
            "Train-->Loss: 0.27128952741622925, epoch: 3610\n",
            "Test --> Loss: 0.27128952741622925, epoch: 3610\n",
            "Train-->Loss: 0.2711743116378784, epoch: 3620\n",
            "Test --> Loss: 0.2711743116378784, epoch: 3620\n",
            "Train-->Loss: 0.27105912566185, epoch: 3630\n",
            "Test --> Loss: 0.27105912566185, epoch: 3630\n",
            "Train-->Loss: 0.27094388008117676, epoch: 3640\n",
            "Test --> Loss: 0.27094388008117676, epoch: 3640\n",
            "Train-->Loss: 0.2708286643028259, epoch: 3650\n",
            "Test --> Loss: 0.2708286643028259, epoch: 3650\n",
            "Train-->Loss: 0.2707134485244751, epoch: 3660\n",
            "Test --> Loss: 0.2707134485244751, epoch: 3660\n",
            "Train-->Loss: 0.27059826254844666, epoch: 3670\n",
            "Test --> Loss: 0.27059826254844666, epoch: 3670\n",
            "Train-->Loss: 0.27048301696777344, epoch: 3680\n",
            "Test --> Loss: 0.27048301696777344, epoch: 3680\n",
            "Train-->Loss: 0.270367830991745, epoch: 3690\n",
            "Test --> Loss: 0.270367830991745, epoch: 3690\n",
            "Train-->Loss: 0.27025264501571655, epoch: 3700\n",
            "Test --> Loss: 0.27025264501571655, epoch: 3700\n",
            "Train-->Loss: 0.27013736963272095, epoch: 3710\n",
            "Test --> Loss: 0.27013736963272095, epoch: 3710\n",
            "Train-->Loss: 0.2700221836566925, epoch: 3720\n",
            "Test --> Loss: 0.2700221836566925, epoch: 3720\n",
            "Train-->Loss: 0.2699069678783417, epoch: 3730\n",
            "Test --> Loss: 0.2699069678783417, epoch: 3730\n",
            "Train-->Loss: 0.26979175209999084, epoch: 3740\n",
            "Test --> Loss: 0.26979175209999084, epoch: 3740\n",
            "Train-->Loss: 0.2696765661239624, epoch: 3750\n",
            "Test --> Loss: 0.2696765661239624, epoch: 3750\n",
            "Train-->Loss: 0.2695613503456116, epoch: 3760\n",
            "Test --> Loss: 0.2695613503456116, epoch: 3760\n",
            "Train-->Loss: 0.26944613456726074, epoch: 3770\n",
            "Test --> Loss: 0.26944613456726074, epoch: 3770\n",
            "Train-->Loss: 0.2693309187889099, epoch: 3780\n",
            "Test --> Loss: 0.2693309187889099, epoch: 3780\n",
            "Train-->Loss: 0.2692157030105591, epoch: 3790\n",
            "Test --> Loss: 0.2692157030105591, epoch: 3790\n",
            "Train-->Loss: 0.26910048723220825, epoch: 3800\n",
            "Test --> Loss: 0.26910048723220825, epoch: 3800\n",
            "Train-->Loss: 0.2689852714538574, epoch: 3810\n",
            "Test --> Loss: 0.2689852714538574, epoch: 3810\n",
            "Train-->Loss: 0.2688700556755066, epoch: 3820\n",
            "Test --> Loss: 0.2688700556755066, epoch: 3820\n",
            "Train-->Loss: 0.26875486969947815, epoch: 3830\n",
            "Test --> Loss: 0.26875486969947815, epoch: 3830\n",
            "Train-->Loss: 0.2686396539211273, epoch: 3840\n",
            "Test --> Loss: 0.2686396539211273, epoch: 3840\n",
            "Train-->Loss: 0.2685244083404541, epoch: 3850\n",
            "Test --> Loss: 0.2685244083404541, epoch: 3850\n",
            "Train-->Loss: 0.26840919256210327, epoch: 3860\n",
            "Test --> Loss: 0.26840919256210327, epoch: 3860\n",
            "Train-->Loss: 0.26829400658607483, epoch: 3870\n",
            "Test --> Loss: 0.26829400658607483, epoch: 3870\n",
            "Train-->Loss: 0.268178790807724, epoch: 3880\n",
            "Test --> Loss: 0.268178790807724, epoch: 3880\n",
            "Train-->Loss: 0.26806357502937317, epoch: 3890\n",
            "Test --> Loss: 0.26806357502937317, epoch: 3890\n",
            "Train-->Loss: 0.26794832944869995, epoch: 3900\n",
            "Test --> Loss: 0.26794832944869995, epoch: 3900\n",
            "Train-->Loss: 0.2678331434726715, epoch: 3910\n",
            "Test --> Loss: 0.2678331434726715, epoch: 3910\n",
            "Train-->Loss: 0.2677179276943207, epoch: 3920\n",
            "Test --> Loss: 0.2677179276943207, epoch: 3920\n",
            "Train-->Loss: 0.26760271191596985, epoch: 3930\n",
            "Test --> Loss: 0.26760271191596985, epoch: 3930\n",
            "Train-->Loss: 0.267487496137619, epoch: 3940\n",
            "Test --> Loss: 0.267487496137619, epoch: 3940\n",
            "Train-->Loss: 0.2673722803592682, epoch: 3950\n",
            "Test --> Loss: 0.2673722803592682, epoch: 3950\n",
            "Train-->Loss: 0.26725706458091736, epoch: 3960\n",
            "Test --> Loss: 0.26725706458091736, epoch: 3960\n",
            "Train-->Loss: 0.2671418786048889, epoch: 3970\n",
            "Test --> Loss: 0.2671418786048889, epoch: 3970\n",
            "Train-->Loss: 0.2670266628265381, epoch: 3980\n",
            "Test --> Loss: 0.2670266628265381, epoch: 3980\n",
            "Train-->Loss: 0.26691144704818726, epoch: 3990\n",
            "Test --> Loss: 0.26691144704818726, epoch: 3990\n",
            "Train-->Loss: 0.26679620146751404, epoch: 4000\n",
            "Test --> Loss: 0.26679620146751404, epoch: 4000\n",
            "Train-->Loss: 0.2666810154914856, epoch: 4010\n",
            "Test --> Loss: 0.2666810154914856, epoch: 4010\n",
            "Train-->Loss: 0.26656579971313477, epoch: 4020\n",
            "Test --> Loss: 0.26656579971313477, epoch: 4020\n",
            "Train-->Loss: 0.26645058393478394, epoch: 4030\n",
            "Test --> Loss: 0.26645058393478394, epoch: 4030\n",
            "Train-->Loss: 0.2663353681564331, epoch: 4040\n",
            "Test --> Loss: 0.2663353681564331, epoch: 4040\n",
            "Train-->Loss: 0.2662201523780823, epoch: 4050\n",
            "Test --> Loss: 0.2662201523780823, epoch: 4050\n",
            "Train-->Loss: 0.26610496640205383, epoch: 4060\n",
            "Test --> Loss: 0.26610496640205383, epoch: 4060\n",
            "Train-->Loss: 0.265989750623703, epoch: 4070\n",
            "Test --> Loss: 0.265989750623703, epoch: 4070\n",
            "Train-->Loss: 0.2658745348453522, epoch: 4080\n",
            "Test --> Loss: 0.2658745348453522, epoch: 4080\n",
            "Train-->Loss: 0.26575931906700134, epoch: 4090\n",
            "Test --> Loss: 0.26575931906700134, epoch: 4090\n",
            "Train-->Loss: 0.2656441032886505, epoch: 4100\n",
            "Test --> Loss: 0.2656441032886505, epoch: 4100\n",
            "Train-->Loss: 0.2655288577079773, epoch: 4110\n",
            "Test --> Loss: 0.2655288577079773, epoch: 4110\n",
            "Train-->Loss: 0.26541370153427124, epoch: 4120\n",
            "Test --> Loss: 0.26541370153427124, epoch: 4120\n",
            "Train-->Loss: 0.265298455953598, epoch: 4130\n",
            "Test --> Loss: 0.265298455953598, epoch: 4130\n",
            "Train-->Loss: 0.2651832699775696, epoch: 4140\n",
            "Test --> Loss: 0.2651832699775696, epoch: 4140\n",
            "Train-->Loss: 0.26506802439689636, epoch: 4150\n",
            "Test --> Loss: 0.26506802439689636, epoch: 4150\n",
            "Train-->Loss: 0.26495280861854553, epoch: 4160\n",
            "Test --> Loss: 0.26495280861854553, epoch: 4160\n",
            "Train-->Loss: 0.2648375928401947, epoch: 4170\n",
            "Test --> Loss: 0.2648375928401947, epoch: 4170\n",
            "Train-->Loss: 0.26472240686416626, epoch: 4180\n",
            "Test --> Loss: 0.26472240686416626, epoch: 4180\n",
            "Train-->Loss: 0.26460716128349304, epoch: 4190\n",
            "Test --> Loss: 0.26460716128349304, epoch: 4190\n",
            "Train-->Loss: 0.2644919753074646, epoch: 4200\n",
            "Test --> Loss: 0.2644919753074646, epoch: 4200\n",
            "Train-->Loss: 0.26437675952911377, epoch: 4210\n",
            "Test --> Loss: 0.26437675952911377, epoch: 4210\n",
            "Train-->Loss: 0.26426154375076294, epoch: 4220\n",
            "Test --> Loss: 0.26426154375076294, epoch: 4220\n",
            "Train-->Loss: 0.2641463577747345, epoch: 4230\n",
            "Test --> Loss: 0.2641463577747345, epoch: 4230\n",
            "Train-->Loss: 0.2640311121940613, epoch: 4240\n",
            "Test --> Loss: 0.2640311121940613, epoch: 4240\n",
            "Train-->Loss: 0.26391592621803284, epoch: 4250\n",
            "Test --> Loss: 0.26391592621803284, epoch: 4250\n",
            "Train-->Loss: 0.263800710439682, epoch: 4260\n",
            "Test --> Loss: 0.263800710439682, epoch: 4260\n",
            "Train-->Loss: 0.26368552446365356, epoch: 4270\n",
            "Test --> Loss: 0.26368552446365356, epoch: 4270\n",
            "Train-->Loss: 0.26357030868530273, epoch: 4280\n",
            "Test --> Loss: 0.26357030868530273, epoch: 4280\n",
            "Train-->Loss: 0.2634550929069519, epoch: 4290\n",
            "Test --> Loss: 0.2634550929069519, epoch: 4290\n",
            "Train-->Loss: 0.2633398175239563, epoch: 4300\n",
            "Test --> Loss: 0.2633398175239563, epoch: 4300\n",
            "Train-->Loss: 0.26322466135025024, epoch: 4310\n",
            "Test --> Loss: 0.26322466135025024, epoch: 4310\n",
            "Train-->Loss: 0.26310938596725464, epoch: 4320\n",
            "Test --> Loss: 0.26310938596725464, epoch: 4320\n",
            "Train-->Loss: 0.2629942297935486, epoch: 4330\n",
            "Test --> Loss: 0.2629942297935486, epoch: 4330\n",
            "Train-->Loss: 0.26287898421287537, epoch: 4340\n",
            "Test --> Loss: 0.26287898421287537, epoch: 4340\n",
            "Train-->Loss: 0.26276376843452454, epoch: 4350\n",
            "Test --> Loss: 0.26276376843452454, epoch: 4350\n",
            "Train-->Loss: 0.2626485526561737, epoch: 4360\n",
            "Test --> Loss: 0.2626485526561737, epoch: 4360\n",
            "Train-->Loss: 0.26253336668014526, epoch: 4370\n",
            "Test --> Loss: 0.26253336668014526, epoch: 4370\n",
            "Train-->Loss: 0.26241815090179443, epoch: 4380\n",
            "Test --> Loss: 0.26241815090179443, epoch: 4380\n",
            "Train-->Loss: 0.2623029351234436, epoch: 4390\n",
            "Test --> Loss: 0.2623029351234436, epoch: 4390\n",
            "Train-->Loss: 0.2621877193450928, epoch: 4400\n",
            "Test --> Loss: 0.2621877193450928, epoch: 4400\n",
            "Train-->Loss: 0.26207250356674194, epoch: 4410\n",
            "Test --> Loss: 0.26207250356674194, epoch: 4410\n",
            "Train-->Loss: 0.2619572877883911, epoch: 4420\n",
            "Test --> Loss: 0.2619572877883911, epoch: 4420\n",
            "Train-->Loss: 0.26184210181236267, epoch: 4430\n",
            "Test --> Loss: 0.26184210181236267, epoch: 4430\n",
            "Train-->Loss: 0.26172685623168945, epoch: 4440\n",
            "Test --> Loss: 0.26172685623168945, epoch: 4440\n",
            "Train-->Loss: 0.2616116404533386, epoch: 4450\n",
            "Test --> Loss: 0.2616116404533386, epoch: 4450\n",
            "Train-->Loss: 0.2614964246749878, epoch: 4460\n",
            "Test --> Loss: 0.2614964246749878, epoch: 4460\n",
            "Train-->Loss: 0.26138123869895935, epoch: 4470\n",
            "Test --> Loss: 0.26138123869895935, epoch: 4470\n",
            "Train-->Loss: 0.26126599311828613, epoch: 4480\n",
            "Test --> Loss: 0.26126599311828613, epoch: 4480\n",
            "Train-->Loss: 0.2611507773399353, epoch: 4490\n",
            "Test --> Loss: 0.2611507773399353, epoch: 4490\n",
            "Train-->Loss: 0.26103559136390686, epoch: 4500\n",
            "Test --> Loss: 0.26103559136390686, epoch: 4500\n",
            "Train-->Loss: 0.26092034578323364, epoch: 4510\n",
            "Test --> Loss: 0.26092034578323364, epoch: 4510\n",
            "Train-->Loss: 0.2608051896095276, epoch: 4520\n",
            "Test --> Loss: 0.2608051896095276, epoch: 4520\n",
            "Train-->Loss: 0.26068994402885437, epoch: 4530\n",
            "Test --> Loss: 0.26068994402885437, epoch: 4530\n",
            "Train-->Loss: 0.2605747580528259, epoch: 4540\n",
            "Test --> Loss: 0.2605747580528259, epoch: 4540\n",
            "Train-->Loss: 0.2604595422744751, epoch: 4550\n",
            "Test --> Loss: 0.2604595422744751, epoch: 4550\n",
            "Train-->Loss: 0.26034432649612427, epoch: 4560\n",
            "Test --> Loss: 0.26034432649612427, epoch: 4560\n",
            "Train-->Loss: 0.26022911071777344, epoch: 4570\n",
            "Test --> Loss: 0.26022911071777344, epoch: 4570\n",
            "Train-->Loss: 0.2601138949394226, epoch: 4580\n",
            "Test --> Loss: 0.2601138949394226, epoch: 4580\n",
            "Train-->Loss: 0.2599986791610718, epoch: 4590\n",
            "Test --> Loss: 0.2599986791610718, epoch: 4590\n",
            "Train-->Loss: 0.25988346338272095, epoch: 4600\n",
            "Test --> Loss: 0.25988346338272095, epoch: 4600\n",
            "Train-->Loss: 0.2597682476043701, epoch: 4610\n",
            "Test --> Loss: 0.2597682476043701, epoch: 4610\n",
            "Train-->Loss: 0.2596530318260193, epoch: 4620\n",
            "Test --> Loss: 0.2596530318260193, epoch: 4620\n",
            "Train-->Loss: 0.25953784584999084, epoch: 4630\n",
            "Test --> Loss: 0.25953784584999084, epoch: 4630\n",
            "Train-->Loss: 0.2594226002693176, epoch: 4640\n",
            "Test --> Loss: 0.2594226002693176, epoch: 4640\n",
            "Train-->Loss: 0.2593074142932892, epoch: 4650\n",
            "Test --> Loss: 0.2593074142932892, epoch: 4650\n",
            "Train-->Loss: 0.25919219851493835, epoch: 4660\n",
            "Test --> Loss: 0.25919219851493835, epoch: 4660\n",
            "Train-->Loss: 0.2590769827365875, epoch: 4670\n",
            "Test --> Loss: 0.2590769827365875, epoch: 4670\n",
            "Train-->Loss: 0.2589617669582367, epoch: 4680\n",
            "Test --> Loss: 0.2589617669582367, epoch: 4680\n",
            "Train-->Loss: 0.25884655117988586, epoch: 4690\n",
            "Test --> Loss: 0.25884655117988586, epoch: 4690\n",
            "Train-->Loss: 0.25873130559921265, epoch: 4700\n",
            "Test --> Loss: 0.25873130559921265, epoch: 4700\n",
            "Train-->Loss: 0.2586161196231842, epoch: 4710\n",
            "Test --> Loss: 0.2586161196231842, epoch: 4710\n",
            "Train-->Loss: 0.2585009038448334, epoch: 4720\n",
            "Test --> Loss: 0.2585009038448334, epoch: 4720\n",
            "Train-->Loss: 0.25838571786880493, epoch: 4730\n",
            "Test --> Loss: 0.25838571786880493, epoch: 4730\n",
            "Train-->Loss: 0.2582704722881317, epoch: 4740\n",
            "Test --> Loss: 0.2582704722881317, epoch: 4740\n",
            "Train-->Loss: 0.2581552565097809, epoch: 4750\n",
            "Test --> Loss: 0.2581552565097809, epoch: 4750\n",
            "Train-->Loss: 0.25804007053375244, epoch: 4760\n",
            "Test --> Loss: 0.25804007053375244, epoch: 4760\n",
            "Train-->Loss: 0.2579248547554016, epoch: 4770\n",
            "Test --> Loss: 0.2579248547554016, epoch: 4770\n",
            "Train-->Loss: 0.2578096389770508, epoch: 4780\n",
            "Test --> Loss: 0.2578096389770508, epoch: 4780\n",
            "Train-->Loss: 0.25769442319869995, epoch: 4790\n",
            "Test --> Loss: 0.25769442319869995, epoch: 4790\n",
            "Train-->Loss: 0.25757917761802673, epoch: 4800\n",
            "Test --> Loss: 0.25757917761802673, epoch: 4800\n",
            "Train-->Loss: 0.2574639916419983, epoch: 4810\n",
            "Test --> Loss: 0.2574639916419983, epoch: 4810\n",
            "Train-->Loss: 0.25734877586364746, epoch: 4820\n",
            "Test --> Loss: 0.25734877586364746, epoch: 4820\n",
            "Train-->Loss: 0.25723356008529663, epoch: 4830\n",
            "Test --> Loss: 0.25723356008529663, epoch: 4830\n",
            "Train-->Loss: 0.2571183741092682, epoch: 4840\n",
            "Test --> Loss: 0.2571183741092682, epoch: 4840\n",
            "Train-->Loss: 0.25700312852859497, epoch: 4850\n",
            "Test --> Loss: 0.25700312852859497, epoch: 4850\n",
            "Train-->Loss: 0.25688791275024414, epoch: 4860\n",
            "Test --> Loss: 0.25688791275024414, epoch: 4860\n",
            "Train-->Loss: 0.2567727267742157, epoch: 4870\n",
            "Test --> Loss: 0.2567727267742157, epoch: 4870\n",
            "Train-->Loss: 0.25665751099586487, epoch: 4880\n",
            "Test --> Loss: 0.25665751099586487, epoch: 4880\n",
            "Train-->Loss: 0.25654226541519165, epoch: 4890\n",
            "Test --> Loss: 0.25654226541519165, epoch: 4890\n",
            "Train-->Loss: 0.2564271092414856, epoch: 4900\n",
            "Test --> Loss: 0.2564271092414856, epoch: 4900\n",
            "Train-->Loss: 0.2563118636608124, epoch: 4910\n",
            "Test --> Loss: 0.2563118636608124, epoch: 4910\n",
            "Train-->Loss: 0.25619664788246155, epoch: 4920\n",
            "Test --> Loss: 0.25619664788246155, epoch: 4920\n",
            "Train-->Loss: 0.2560814321041107, epoch: 4930\n",
            "Test --> Loss: 0.2560814321041107, epoch: 4930\n",
            "Train-->Loss: 0.2559662461280823, epoch: 4940\n",
            "Test --> Loss: 0.2559662461280823, epoch: 4940\n",
            "Train-->Loss: 0.25585103034973145, epoch: 4950\n",
            "Test --> Loss: 0.25585103034973145, epoch: 4950\n",
            "Train-->Loss: 0.2557357847690582, epoch: 4960\n",
            "Test --> Loss: 0.2557357847690582, epoch: 4960\n",
            "Train-->Loss: 0.2556205689907074, epoch: 4970\n",
            "Test --> Loss: 0.2556205689907074, epoch: 4970\n",
            "Train-->Loss: 0.25550538301467896, epoch: 4980\n",
            "Test --> Loss: 0.25550538301467896, epoch: 4980\n",
            "Train-->Loss: 0.25539013743400574, epoch: 4990\n",
            "Test --> Loss: 0.25539013743400574, epoch: 4990\n",
            "Train-->Loss: 0.2552749514579773, epoch: 5000\n",
            "Test --> Loss: 0.2552749514579773, epoch: 5000\n",
            "Train-->Loss: 0.25515973567962646, epoch: 5010\n",
            "Test --> Loss: 0.25515973567962646, epoch: 5010\n",
            "Train-->Loss: 0.25504451990127563, epoch: 5020\n",
            "Test --> Loss: 0.25504451990127563, epoch: 5020\n",
            "Train-->Loss: 0.2549293339252472, epoch: 5030\n",
            "Test --> Loss: 0.2549293339252472, epoch: 5030\n",
            "Train-->Loss: 0.254814088344574, epoch: 5040\n",
            "Test --> Loss: 0.254814088344574, epoch: 5040\n",
            "Train-->Loss: 0.25469890236854553, epoch: 5050\n",
            "Test --> Loss: 0.25469890236854553, epoch: 5050\n",
            "Train-->Loss: 0.2545836567878723, epoch: 5060\n",
            "Test --> Loss: 0.2545836567878723, epoch: 5060\n",
            "Train-->Loss: 0.25446850061416626, epoch: 5070\n",
            "Test --> Loss: 0.25446850061416626, epoch: 5070\n",
            "Train-->Loss: 0.25435325503349304, epoch: 5080\n",
            "Test --> Loss: 0.25435325503349304, epoch: 5080\n",
            "Train-->Loss: 0.2542380392551422, epoch: 5090\n",
            "Test --> Loss: 0.2542380392551422, epoch: 5090\n",
            "Train-->Loss: 0.254122793674469, epoch: 5100\n",
            "Test --> Loss: 0.254122793674469, epoch: 5100\n",
            "Train-->Loss: 0.25400760769844055, epoch: 5110\n",
            "Test --> Loss: 0.25400760769844055, epoch: 5110\n",
            "Train-->Loss: 0.25389236211776733, epoch: 5120\n",
            "Test --> Loss: 0.25389236211776733, epoch: 5120\n",
            "Train-->Loss: 0.2537771761417389, epoch: 5130\n",
            "Test --> Loss: 0.2537771761417389, epoch: 5130\n",
            "Train-->Loss: 0.25366196036338806, epoch: 5140\n",
            "Test --> Loss: 0.25366196036338806, epoch: 5140\n",
            "Train-->Loss: 0.25354674458503723, epoch: 5150\n",
            "Test --> Loss: 0.25354674458503723, epoch: 5150\n",
            "Train-->Loss: 0.2534315586090088, epoch: 5160\n",
            "Test --> Loss: 0.2534315586090088, epoch: 5160\n",
            "Train-->Loss: 0.25331634283065796, epoch: 5170\n",
            "Test --> Loss: 0.25331634283065796, epoch: 5170\n",
            "Train-->Loss: 0.25320112705230713, epoch: 5180\n",
            "Test --> Loss: 0.25320112705230713, epoch: 5180\n",
            "Train-->Loss: 0.2530859112739563, epoch: 5190\n",
            "Test --> Loss: 0.2530859112739563, epoch: 5190\n",
            "Train-->Loss: 0.25297069549560547, epoch: 5200\n",
            "Test --> Loss: 0.25297069549560547, epoch: 5200\n",
            "Train-->Loss: 0.25285547971725464, epoch: 5210\n",
            "Test --> Loss: 0.25285547971725464, epoch: 5210\n",
            "Train-->Loss: 0.2527402639389038, epoch: 5220\n",
            "Test --> Loss: 0.2527402639389038, epoch: 5220\n",
            "Train-->Loss: 0.25262507796287537, epoch: 5230\n",
            "Test --> Loss: 0.25262507796287537, epoch: 5230\n",
            "Train-->Loss: 0.25250983238220215, epoch: 5240\n",
            "Test --> Loss: 0.25250983238220215, epoch: 5240\n",
            "Train-->Loss: 0.2523946166038513, epoch: 5250\n",
            "Test --> Loss: 0.2523946166038513, epoch: 5250\n",
            "Train-->Loss: 0.2522794008255005, epoch: 5260\n",
            "Test --> Loss: 0.2522794008255005, epoch: 5260\n",
            "Train-->Loss: 0.25216421484947205, epoch: 5270\n",
            "Test --> Loss: 0.25216421484947205, epoch: 5270\n",
            "Train-->Loss: 0.25204896926879883, epoch: 5280\n",
            "Test --> Loss: 0.25204896926879883, epoch: 5280\n",
            "Train-->Loss: 0.2519337832927704, epoch: 5290\n",
            "Test --> Loss: 0.2519337832927704, epoch: 5290\n",
            "Train-->Loss: 0.25181859731674194, epoch: 5300\n",
            "Test --> Loss: 0.25181859731674194, epoch: 5300\n",
            "Train-->Loss: 0.25170332193374634, epoch: 5310\n",
            "Test --> Loss: 0.25170332193374634, epoch: 5310\n",
            "Train-->Loss: 0.2515881359577179, epoch: 5320\n",
            "Test --> Loss: 0.2515881359577179, epoch: 5320\n",
            "Train-->Loss: 0.25147292017936707, epoch: 5330\n",
            "Test --> Loss: 0.25147292017936707, epoch: 5330\n",
            "Train-->Loss: 0.25135770440101624, epoch: 5340\n",
            "Test --> Loss: 0.25135770440101624, epoch: 5340\n",
            "Train-->Loss: 0.2512425184249878, epoch: 5350\n",
            "Test --> Loss: 0.2512425184249878, epoch: 5350\n",
            "Train-->Loss: 0.25112730264663696, epoch: 5360\n",
            "Test --> Loss: 0.25112730264663696, epoch: 5360\n",
            "Train-->Loss: 0.25101208686828613, epoch: 5370\n",
            "Test --> Loss: 0.25101208686828613, epoch: 5370\n",
            "Train-->Loss: 0.2508968710899353, epoch: 5380\n",
            "Test --> Loss: 0.2508968710899353, epoch: 5380\n",
            "Train-->Loss: 0.2507816553115845, epoch: 5390\n",
            "Test --> Loss: 0.2507816553115845, epoch: 5390\n",
            "Train-->Loss: 0.25066643953323364, epoch: 5400\n",
            "Test --> Loss: 0.25066643953323364, epoch: 5400\n",
            "Train-->Loss: 0.2505512237548828, epoch: 5410\n",
            "Test --> Loss: 0.2505512237548828, epoch: 5410\n",
            "Train-->Loss: 0.250436007976532, epoch: 5420\n",
            "Test --> Loss: 0.250436007976532, epoch: 5420\n",
            "Train-->Loss: 0.25032082200050354, epoch: 5430\n",
            "Test --> Loss: 0.25032082200050354, epoch: 5430\n",
            "Train-->Loss: 0.2502056062221527, epoch: 5440\n",
            "Test --> Loss: 0.2502056062221527, epoch: 5440\n",
            "Train-->Loss: 0.2500903606414795, epoch: 5450\n",
            "Test --> Loss: 0.2500903606414795, epoch: 5450\n",
            "Train-->Loss: 0.24997515976428986, epoch: 5460\n",
            "Test --> Loss: 0.24997515976428986, epoch: 5460\n",
            "Train-->Loss: 0.24985995888710022, epoch: 5470\n",
            "Test --> Loss: 0.24985995888710022, epoch: 5470\n",
            "Train-->Loss: 0.2497447431087494, epoch: 5480\n",
            "Test --> Loss: 0.2497447431087494, epoch: 5480\n",
            "Train-->Loss: 0.24962952733039856, epoch: 5490\n",
            "Test --> Loss: 0.24962952733039856, epoch: 5490\n",
            "Train-->Loss: 0.24951429665088654, epoch: 5500\n",
            "Test --> Loss: 0.24951429665088654, epoch: 5500\n",
            "Train-->Loss: 0.2493991106748581, epoch: 5510\n",
            "Test --> Loss: 0.2493991106748581, epoch: 5510\n",
            "Train-->Loss: 0.24928387999534607, epoch: 5520\n",
            "Test --> Loss: 0.24928387999534607, epoch: 5520\n",
            "Train-->Loss: 0.24916866421699524, epoch: 5530\n",
            "Test --> Loss: 0.24916866421699524, epoch: 5530\n",
            "Train-->Loss: 0.2490534484386444, epoch: 5540\n",
            "Test --> Loss: 0.2490534484386444, epoch: 5540\n",
            "Train-->Loss: 0.24893823266029358, epoch: 5550\n",
            "Test --> Loss: 0.24893823266029358, epoch: 5550\n",
            "Train-->Loss: 0.24882301688194275, epoch: 5560\n",
            "Test --> Loss: 0.24882301688194275, epoch: 5560\n",
            "Train-->Loss: 0.2487078160047531, epoch: 5570\n",
            "Test --> Loss: 0.2487078160047531, epoch: 5570\n",
            "Train-->Loss: 0.24859261512756348, epoch: 5580\n",
            "Test --> Loss: 0.24859261512756348, epoch: 5580\n",
            "Train-->Loss: 0.24847741425037384, epoch: 5590\n",
            "Test --> Loss: 0.24847741425037384, epoch: 5590\n",
            "Train-->Loss: 0.24836215376853943, epoch: 5600\n",
            "Test --> Loss: 0.24836215376853943, epoch: 5600\n",
            "Train-->Loss: 0.24824698269367218, epoch: 5610\n",
            "Test --> Loss: 0.24824698269367218, epoch: 5610\n",
            "Train-->Loss: 0.24813175201416016, epoch: 5620\n",
            "Test --> Loss: 0.24813175201416016, epoch: 5620\n",
            "Train-->Loss: 0.24801655113697052, epoch: 5630\n",
            "Test --> Loss: 0.24801655113697052, epoch: 5630\n",
            "Train-->Loss: 0.2479013204574585, epoch: 5640\n",
            "Test --> Loss: 0.2479013204574585, epoch: 5640\n",
            "Train-->Loss: 0.24778611958026886, epoch: 5650\n",
            "Test --> Loss: 0.24778611958026886, epoch: 5650\n",
            "Train-->Loss: 0.24767091870307922, epoch: 5660\n",
            "Test --> Loss: 0.24767091870307922, epoch: 5660\n",
            "Train-->Loss: 0.2475557029247284, epoch: 5670\n",
            "Test --> Loss: 0.2475557029247284, epoch: 5670\n",
            "Train-->Loss: 0.24744048714637756, epoch: 5680\n",
            "Test --> Loss: 0.24744048714637756, epoch: 5680\n",
            "Train-->Loss: 0.24732527136802673, epoch: 5690\n",
            "Test --> Loss: 0.24732527136802673, epoch: 5690\n",
            "Train-->Loss: 0.2472100555896759, epoch: 5700\n",
            "Test --> Loss: 0.2472100555896759, epoch: 5700\n",
            "Train-->Loss: 0.24709482491016388, epoch: 5710\n",
            "Test --> Loss: 0.24709482491016388, epoch: 5710\n",
            "Train-->Loss: 0.24697963893413544, epoch: 5720\n",
            "Test --> Loss: 0.24697963893413544, epoch: 5720\n",
            "Train-->Loss: 0.2468644082546234, epoch: 5730\n",
            "Test --> Loss: 0.2468644082546234, epoch: 5730\n",
            "Train-->Loss: 0.24674920737743378, epoch: 5740\n",
            "Test --> Loss: 0.24674920737743378, epoch: 5740\n",
            "Train-->Loss: 0.24663397669792175, epoch: 5750\n",
            "Test --> Loss: 0.24663397669792175, epoch: 5750\n",
            "Train-->Loss: 0.24651876091957092, epoch: 5760\n",
            "Test --> Loss: 0.24651876091957092, epoch: 5760\n",
            "Train-->Loss: 0.2464035451412201, epoch: 5770\n",
            "Test --> Loss: 0.2464035451412201, epoch: 5770\n",
            "Train-->Loss: 0.24628837406635284, epoch: 5780\n",
            "Test --> Loss: 0.24628837406635284, epoch: 5780\n",
            "Train-->Loss: 0.24617311358451843, epoch: 5790\n",
            "Test --> Loss: 0.24617311358451843, epoch: 5790\n",
            "Train-->Loss: 0.24605794250965118, epoch: 5800\n",
            "Test --> Loss: 0.24605794250965118, epoch: 5800\n",
            "Train-->Loss: 0.24594271183013916, epoch: 5810\n",
            "Test --> Loss: 0.24594271183013916, epoch: 5810\n",
            "Train-->Loss: 0.24582751095294952, epoch: 5820\n",
            "Test --> Loss: 0.24582751095294952, epoch: 5820\n",
            "Train-->Loss: 0.2457123100757599, epoch: 5830\n",
            "Test --> Loss: 0.2457123100757599, epoch: 5830\n",
            "Train-->Loss: 0.24559707939624786, epoch: 5840\n",
            "Test --> Loss: 0.24559707939624786, epoch: 5840\n",
            "Train-->Loss: 0.24548187851905823, epoch: 5850\n",
            "Test --> Loss: 0.24548187851905823, epoch: 5850\n",
            "Train-->Loss: 0.2453666627407074, epoch: 5860\n",
            "Test --> Loss: 0.2453666627407074, epoch: 5860\n",
            "Train-->Loss: 0.24525146186351776, epoch: 5870\n",
            "Test --> Loss: 0.24525146186351776, epoch: 5870\n",
            "Train-->Loss: 0.24513626098632812, epoch: 5880\n",
            "Test --> Loss: 0.24513626098632812, epoch: 5880\n",
            "Train-->Loss: 0.2450210154056549, epoch: 5890\n",
            "Test --> Loss: 0.2450210154056549, epoch: 5890\n",
            "Train-->Loss: 0.24490578472614288, epoch: 5900\n",
            "Test --> Loss: 0.24490578472614288, epoch: 5900\n",
            "Train-->Loss: 0.24479059875011444, epoch: 5910\n",
            "Test --> Loss: 0.24479059875011444, epoch: 5910\n",
            "Train-->Loss: 0.24467535316944122, epoch: 5920\n",
            "Test --> Loss: 0.24467535316944122, epoch: 5920\n",
            "Train-->Loss: 0.24456016719341278, epoch: 5930\n",
            "Test --> Loss: 0.24456016719341278, epoch: 5930\n",
            "Train-->Loss: 0.24444493651390076, epoch: 5940\n",
            "Test --> Loss: 0.24444493651390076, epoch: 5940\n",
            "Train-->Loss: 0.24432972073554993, epoch: 5950\n",
            "Test --> Loss: 0.24432972073554993, epoch: 5950\n",
            "Train-->Loss: 0.2442145049571991, epoch: 5960\n",
            "Test --> Loss: 0.2442145049571991, epoch: 5960\n",
            "Train-->Loss: 0.24409930408000946, epoch: 5970\n",
            "Test --> Loss: 0.24409930408000946, epoch: 5970\n",
            "Train-->Loss: 0.24398410320281982, epoch: 5980\n",
            "Test --> Loss: 0.24398410320281982, epoch: 5980\n",
            "Train-->Loss: 0.2438689023256302, epoch: 5990\n",
            "Test --> Loss: 0.2438689023256302, epoch: 5990\n",
            "Train-->Loss: 0.24375367164611816, epoch: 6000\n",
            "Test --> Loss: 0.24375367164611816, epoch: 6000\n",
            "Train-->Loss: 0.24363847076892853, epoch: 6010\n",
            "Test --> Loss: 0.24363847076892853, epoch: 6010\n",
            "Train-->Loss: 0.2435232400894165, epoch: 6020\n",
            "Test --> Loss: 0.2435232400894165, epoch: 6020\n",
            "Train-->Loss: 0.24340805411338806, epoch: 6030\n",
            "Test --> Loss: 0.24340805411338806, epoch: 6030\n",
            "Train-->Loss: 0.24329280853271484, epoch: 6040\n",
            "Test --> Loss: 0.24329280853271484, epoch: 6040\n",
            "Train-->Loss: 0.2431776076555252, epoch: 6050\n",
            "Test --> Loss: 0.2431776076555252, epoch: 6050\n",
            "Train-->Loss: 0.24306237697601318, epoch: 6060\n",
            "Test --> Loss: 0.24306237697601318, epoch: 6060\n",
            "Train-->Loss: 0.24294719099998474, epoch: 6070\n",
            "Test --> Loss: 0.24294719099998474, epoch: 6070\n",
            "Train-->Loss: 0.2428319901227951, epoch: 6080\n",
            "Test --> Loss: 0.2428319901227951, epoch: 6080\n",
            "Train-->Loss: 0.2427167445421219, epoch: 6090\n",
            "Test --> Loss: 0.2427167445421219, epoch: 6090\n",
            "Train-->Loss: 0.24260154366493225, epoch: 6100\n",
            "Test --> Loss: 0.24260154366493225, epoch: 6100\n",
            "Train-->Loss: 0.24248631298542023, epoch: 6110\n",
            "Test --> Loss: 0.24248631298542023, epoch: 6110\n",
            "Train-->Loss: 0.24237112700939178, epoch: 6120\n",
            "Test --> Loss: 0.24237112700939178, epoch: 6120\n",
            "Train-->Loss: 0.24225589632987976, epoch: 6130\n",
            "Test --> Loss: 0.24225589632987976, epoch: 6130\n",
            "Train-->Loss: 0.24214069545269012, epoch: 6140\n",
            "Test --> Loss: 0.24214069545269012, epoch: 6140\n",
            "Train-->Loss: 0.2420254945755005, epoch: 6150\n",
            "Test --> Loss: 0.2420254945755005, epoch: 6150\n",
            "Train-->Loss: 0.24191026389598846, epoch: 6160\n",
            "Test --> Loss: 0.24191026389598846, epoch: 6160\n",
            "Train-->Loss: 0.24179503321647644, epoch: 6170\n",
            "Test --> Loss: 0.24179503321647644, epoch: 6170\n",
            "Train-->Loss: 0.2416798621416092, epoch: 6180\n",
            "Test --> Loss: 0.2416798621416092, epoch: 6180\n",
            "Train-->Loss: 0.24156463146209717, epoch: 6190\n",
            "Test --> Loss: 0.24156463146209717, epoch: 6190\n",
            "Train-->Loss: 0.24144940078258514, epoch: 6200\n",
            "Test --> Loss: 0.24144940078258514, epoch: 6200\n",
            "Train-->Loss: 0.2413341999053955, epoch: 6210\n",
            "Test --> Loss: 0.2413341999053955, epoch: 6210\n",
            "Train-->Loss: 0.24121899902820587, epoch: 6220\n",
            "Test --> Loss: 0.24121899902820587, epoch: 6220\n",
            "Train-->Loss: 0.24110379815101624, epoch: 6230\n",
            "Test --> Loss: 0.24110379815101624, epoch: 6230\n",
            "Train-->Loss: 0.24098853766918182, epoch: 6240\n",
            "Test --> Loss: 0.24098853766918182, epoch: 6240\n",
            "Train-->Loss: 0.24087336659431458, epoch: 6250\n",
            "Test --> Loss: 0.24087336659431458, epoch: 6250\n",
            "Train-->Loss: 0.24075815081596375, epoch: 6260\n",
            "Test --> Loss: 0.24075815081596375, epoch: 6260\n",
            "Train-->Loss: 0.24064293503761292, epoch: 6270\n",
            "Test --> Loss: 0.24064293503761292, epoch: 6270\n",
            "Train-->Loss: 0.24052771925926208, epoch: 6280\n",
            "Test --> Loss: 0.24052771925926208, epoch: 6280\n",
            "Train-->Loss: 0.24041250348091125, epoch: 6290\n",
            "Test --> Loss: 0.24041250348091125, epoch: 6290\n",
            "Train-->Loss: 0.24029728770256042, epoch: 6300\n",
            "Test --> Loss: 0.24029728770256042, epoch: 6300\n",
            "Train-->Loss: 0.2401820719242096, epoch: 6310\n",
            "Test --> Loss: 0.2401820719242096, epoch: 6310\n",
            "Train-->Loss: 0.24006685614585876, epoch: 6320\n",
            "Test --> Loss: 0.24006685614585876, epoch: 6320\n",
            "Train-->Loss: 0.23995165526866913, epoch: 6330\n",
            "Test --> Loss: 0.23995165526866913, epoch: 6330\n",
            "Train-->Loss: 0.2398364245891571, epoch: 6340\n",
            "Test --> Loss: 0.2398364245891571, epoch: 6340\n",
            "Train-->Loss: 0.23972120881080627, epoch: 6350\n",
            "Test --> Loss: 0.23972120881080627, epoch: 6350\n",
            "Train-->Loss: 0.23960602283477783, epoch: 6360\n",
            "Test --> Loss: 0.23960602283477783, epoch: 6360\n",
            "Train-->Loss: 0.2394907921552658, epoch: 6370\n",
            "Test --> Loss: 0.2394907921552658, epoch: 6370\n",
            "Train-->Loss: 0.23937559127807617, epoch: 6380\n",
            "Test --> Loss: 0.23937559127807617, epoch: 6380\n",
            "Train-->Loss: 0.23926039040088654, epoch: 6390\n",
            "Test --> Loss: 0.23926039040088654, epoch: 6390\n",
            "Train-->Loss: 0.23914512991905212, epoch: 6400\n",
            "Test --> Loss: 0.23914512991905212, epoch: 6400\n",
            "Train-->Loss: 0.2390299290418625, epoch: 6410\n",
            "Test --> Loss: 0.2390299290418625, epoch: 6410\n",
            "Train-->Loss: 0.23891472816467285, epoch: 6420\n",
            "Test --> Loss: 0.23891472816467285, epoch: 6420\n",
            "Train-->Loss: 0.23879949748516083, epoch: 6430\n",
            "Test --> Loss: 0.23879949748516083, epoch: 6430\n",
            "Train-->Loss: 0.23868432641029358, epoch: 6440\n",
            "Test --> Loss: 0.23868432641029358, epoch: 6440\n",
            "Train-->Loss: 0.23856909573078156, epoch: 6450\n",
            "Test --> Loss: 0.23856909573078156, epoch: 6450\n",
            "Train-->Loss: 0.23845386505126953, epoch: 6460\n",
            "Test --> Loss: 0.23845386505126953, epoch: 6460\n",
            "Train-->Loss: 0.2383386641740799, epoch: 6470\n",
            "Test --> Loss: 0.2383386641740799, epoch: 6470\n",
            "Train-->Loss: 0.23822346329689026, epoch: 6480\n",
            "Test --> Loss: 0.23822346329689026, epoch: 6480\n",
            "Train-->Loss: 0.23810823261737823, epoch: 6490\n",
            "Test --> Loss: 0.23810823261737823, epoch: 6490\n",
            "Train-->Loss: 0.2379930466413498, epoch: 6500\n",
            "Test --> Loss: 0.2379930466413498, epoch: 6500\n",
            "Train-->Loss: 0.23787781596183777, epoch: 6510\n",
            "Test --> Loss: 0.23787781596183777, epoch: 6510\n",
            "Train-->Loss: 0.23776260018348694, epoch: 6520\n",
            "Test --> Loss: 0.23776260018348694, epoch: 6520\n",
            "Train-->Loss: 0.2376473844051361, epoch: 6530\n",
            "Test --> Loss: 0.2376473844051361, epoch: 6530\n",
            "Train-->Loss: 0.23753218352794647, epoch: 6540\n",
            "Test --> Loss: 0.23753218352794647, epoch: 6540\n",
            "Train-->Loss: 0.23741698265075684, epoch: 6550\n",
            "Test --> Loss: 0.23741698265075684, epoch: 6550\n",
            "Train-->Loss: 0.23730173707008362, epoch: 6560\n",
            "Test --> Loss: 0.23730173707008362, epoch: 6560\n",
            "Train-->Loss: 0.2371865212917328, epoch: 6570\n",
            "Test --> Loss: 0.2371865212917328, epoch: 6570\n",
            "Train-->Loss: 0.23707135021686554, epoch: 6580\n",
            "Test --> Loss: 0.23707135021686554, epoch: 6580\n",
            "Train-->Loss: 0.23695608973503113, epoch: 6590\n",
            "Test --> Loss: 0.23695608973503113, epoch: 6590\n",
            "Train-->Loss: 0.2368408888578415, epoch: 6600\n",
            "Test --> Loss: 0.2368408888578415, epoch: 6600\n",
            "Train-->Loss: 0.23672568798065186, epoch: 6610\n",
            "Test --> Loss: 0.23672568798065186, epoch: 6610\n",
            "Train-->Loss: 0.23661048710346222, epoch: 6620\n",
            "Test --> Loss: 0.23661048710346222, epoch: 6620\n",
            "Train-->Loss: 0.23649528622627258, epoch: 6630\n",
            "Test --> Loss: 0.23649528622627258, epoch: 6630\n",
            "Train-->Loss: 0.23638005554676056, epoch: 6640\n",
            "Test --> Loss: 0.23638005554676056, epoch: 6640\n",
            "Train-->Loss: 0.23626485466957092, epoch: 6650\n",
            "Test --> Loss: 0.23626485466957092, epoch: 6650\n",
            "Train-->Loss: 0.2361496239900589, epoch: 6660\n",
            "Test --> Loss: 0.2361496239900589, epoch: 6660\n",
            "Train-->Loss: 0.23603443801403046, epoch: 6670\n",
            "Test --> Loss: 0.23603443801403046, epoch: 6670\n",
            "Train-->Loss: 0.23591920733451843, epoch: 6680\n",
            "Test --> Loss: 0.23591920733451843, epoch: 6680\n",
            "Train-->Loss: 0.2358039915561676, epoch: 6690\n",
            "Test --> Loss: 0.2358039915561676, epoch: 6690\n",
            "Train-->Loss: 0.23568876087665558, epoch: 6700\n",
            "Test --> Loss: 0.23568876087665558, epoch: 6700\n",
            "Train-->Loss: 0.23557355999946594, epoch: 6710\n",
            "Test --> Loss: 0.23557355999946594, epoch: 6710\n",
            "Train-->Loss: 0.23545832931995392, epoch: 6720\n",
            "Test --> Loss: 0.23545832931995392, epoch: 6720\n",
            "Train-->Loss: 0.23534312844276428, epoch: 6730\n",
            "Test --> Loss: 0.23534312844276428, epoch: 6730\n",
            "Train-->Loss: 0.23522791266441345, epoch: 6740\n",
            "Test --> Loss: 0.23522791266441345, epoch: 6740\n",
            "Train-->Loss: 0.23511269688606262, epoch: 6750\n",
            "Test --> Loss: 0.23511269688606262, epoch: 6750\n",
            "Train-->Loss: 0.23499751091003418, epoch: 6760\n",
            "Test --> Loss: 0.23499751091003418, epoch: 6760\n",
            "Train-->Loss: 0.23488228023052216, epoch: 6770\n",
            "Test --> Loss: 0.23488228023052216, epoch: 6770\n",
            "Train-->Loss: 0.23476707935333252, epoch: 6780\n",
            "Test --> Loss: 0.23476707935333252, epoch: 6780\n",
            "Train-->Loss: 0.2346518486738205, epoch: 6790\n",
            "Test --> Loss: 0.2346518486738205, epoch: 6790\n",
            "Train-->Loss: 0.23453664779663086, epoch: 6800\n",
            "Test --> Loss: 0.23453664779663086, epoch: 6800\n",
            "Train-->Loss: 0.23442144691944122, epoch: 6810\n",
            "Test --> Loss: 0.23442144691944122, epoch: 6810\n",
            "Train-->Loss: 0.2343062162399292, epoch: 6820\n",
            "Test --> Loss: 0.2343062162399292, epoch: 6820\n",
            "Train-->Loss: 0.23419101536273956, epoch: 6830\n",
            "Test --> Loss: 0.23419101536273956, epoch: 6830\n",
            "Train-->Loss: 0.23407578468322754, epoch: 6840\n",
            "Test --> Loss: 0.23407578468322754, epoch: 6840\n",
            "Train-->Loss: 0.2339605838060379, epoch: 6850\n",
            "Test --> Loss: 0.2339605838060379, epoch: 6850\n",
            "Train-->Loss: 0.23384535312652588, epoch: 6860\n",
            "Test --> Loss: 0.23384535312652588, epoch: 6860\n",
            "Train-->Loss: 0.23373016715049744, epoch: 6870\n",
            "Test --> Loss: 0.23373016715049744, epoch: 6870\n",
            "Train-->Loss: 0.23361492156982422, epoch: 6880\n",
            "Test --> Loss: 0.23361492156982422, epoch: 6880\n",
            "Train-->Loss: 0.23349973559379578, epoch: 6890\n",
            "Test --> Loss: 0.23349973559379578, epoch: 6890\n",
            "Train-->Loss: 0.23338453471660614, epoch: 6900\n",
            "Test --> Loss: 0.23338453471660614, epoch: 6900\n",
            "Train-->Loss: 0.23326928913593292, epoch: 6910\n",
            "Test --> Loss: 0.23326928913593292, epoch: 6910\n",
            "Train-->Loss: 0.2331540882587433, epoch: 6920\n",
            "Test --> Loss: 0.2331540882587433, epoch: 6920\n",
            "Train-->Loss: 0.23303887248039246, epoch: 6930\n",
            "Test --> Loss: 0.23303887248039246, epoch: 6930\n",
            "Train-->Loss: 0.23292367160320282, epoch: 6940\n",
            "Test --> Loss: 0.23292367160320282, epoch: 6940\n",
            "Train-->Loss: 0.23280847072601318, epoch: 6950\n",
            "Test --> Loss: 0.23280847072601318, epoch: 6950\n",
            "Train-->Loss: 0.23269324004650116, epoch: 6960\n",
            "Test --> Loss: 0.23269324004650116, epoch: 6960\n",
            "Train-->Loss: 0.23257803916931152, epoch: 6970\n",
            "Test --> Loss: 0.23257803916931152, epoch: 6970\n",
            "Train-->Loss: 0.2324628084897995, epoch: 6980\n",
            "Test --> Loss: 0.2324628084897995, epoch: 6980\n",
            "Train-->Loss: 0.23234760761260986, epoch: 6990\n",
            "Test --> Loss: 0.23234760761260986, epoch: 6990\n",
            "Train-->Loss: 0.23223237693309784, epoch: 7000\n",
            "Test --> Loss: 0.23223237693309784, epoch: 7000\n",
            "Train-->Loss: 0.2321171760559082, epoch: 7010\n",
            "Test --> Loss: 0.2321171760559082, epoch: 7010\n",
            "Train-->Loss: 0.23200197517871857, epoch: 7020\n",
            "Test --> Loss: 0.23200197517871857, epoch: 7020\n",
            "Train-->Loss: 0.23188677430152893, epoch: 7030\n",
            "Test --> Loss: 0.23188677430152893, epoch: 7030\n",
            "Train-->Loss: 0.2317715585231781, epoch: 7040\n",
            "Test --> Loss: 0.2317715585231781, epoch: 7040\n",
            "Train-->Loss: 0.23165634274482727, epoch: 7050\n",
            "Test --> Loss: 0.23165634274482727, epoch: 7050\n",
            "Train-->Loss: 0.23154111206531525, epoch: 7060\n",
            "Test --> Loss: 0.23154111206531525, epoch: 7060\n",
            "Train-->Loss: 0.2314259111881256, epoch: 7070\n",
            "Test --> Loss: 0.2314259111881256, epoch: 7070\n",
            "Train-->Loss: 0.23131069540977478, epoch: 7080\n",
            "Test --> Loss: 0.23131069540977478, epoch: 7080\n",
            "Train-->Loss: 0.23119547963142395, epoch: 7090\n",
            "Test --> Loss: 0.23119547963142395, epoch: 7090\n",
            "Train-->Loss: 0.23108024895191193, epoch: 7100\n",
            "Test --> Loss: 0.23108024895191193, epoch: 7100\n",
            "Train-->Loss: 0.23096506297588348, epoch: 7110\n",
            "Test --> Loss: 0.23096506297588348, epoch: 7110\n",
            "Train-->Loss: 0.23084983229637146, epoch: 7120\n",
            "Test --> Loss: 0.23084983229637146, epoch: 7120\n",
            "Train-->Loss: 0.23073461651802063, epoch: 7130\n",
            "Test --> Loss: 0.23073461651802063, epoch: 7130\n",
            "Train-->Loss: 0.2306194007396698, epoch: 7140\n",
            "Test --> Loss: 0.2306194007396698, epoch: 7140\n",
            "Train-->Loss: 0.23050418496131897, epoch: 7150\n",
            "Test --> Loss: 0.23050418496131897, epoch: 7150\n",
            "Train-->Loss: 0.23038896918296814, epoch: 7160\n",
            "Test --> Loss: 0.23038896918296814, epoch: 7160\n",
            "Train-->Loss: 0.2302737683057785, epoch: 7170\n",
            "Test --> Loss: 0.2302737683057785, epoch: 7170\n",
            "Train-->Loss: 0.23015856742858887, epoch: 7180\n",
            "Test --> Loss: 0.23015856742858887, epoch: 7180\n",
            "Train-->Loss: 0.23004333674907684, epoch: 7190\n",
            "Test --> Loss: 0.23004333674907684, epoch: 7190\n",
            "Train-->Loss: 0.22992810606956482, epoch: 7200\n",
            "Test --> Loss: 0.22992810606956482, epoch: 7200\n",
            "Train-->Loss: 0.22981293499469757, epoch: 7210\n",
            "Test --> Loss: 0.22981293499469757, epoch: 7210\n",
            "Train-->Loss: 0.22969770431518555, epoch: 7220\n",
            "Test --> Loss: 0.22969770431518555, epoch: 7220\n",
            "Train-->Loss: 0.2295825034379959, epoch: 7230\n",
            "Test --> Loss: 0.2295825034379959, epoch: 7230\n",
            "Train-->Loss: 0.2294672727584839, epoch: 7240\n",
            "Test --> Loss: 0.2294672727584839, epoch: 7240\n",
            "Train-->Loss: 0.22935207188129425, epoch: 7250\n",
            "Test --> Loss: 0.22935207188129425, epoch: 7250\n",
            "Train-->Loss: 0.22923687100410461, epoch: 7260\n",
            "Test --> Loss: 0.22923687100410461, epoch: 7260\n",
            "Train-->Loss: 0.22912165522575378, epoch: 7270\n",
            "Test --> Loss: 0.22912165522575378, epoch: 7270\n",
            "Train-->Loss: 0.22900643944740295, epoch: 7280\n",
            "Test --> Loss: 0.22900643944740295, epoch: 7280\n",
            "Train-->Loss: 0.22889122366905212, epoch: 7290\n",
            "Test --> Loss: 0.22889122366905212, epoch: 7290\n",
            "Train-->Loss: 0.2287760078907013, epoch: 7300\n",
            "Test --> Loss: 0.2287760078907013, epoch: 7300\n",
            "Train-->Loss: 0.22866077721118927, epoch: 7310\n",
            "Test --> Loss: 0.22866077721118927, epoch: 7310\n",
            "Train-->Loss: 0.22854559123516083, epoch: 7320\n",
            "Test --> Loss: 0.22854559123516083, epoch: 7320\n",
            "Train-->Loss: 0.2284303605556488, epoch: 7330\n",
            "Test --> Loss: 0.2284303605556488, epoch: 7330\n",
            "Train-->Loss: 0.22831515967845917, epoch: 7340\n",
            "Test --> Loss: 0.22831515967845917, epoch: 7340\n",
            "Train-->Loss: 0.22819992899894714, epoch: 7350\n",
            "Test --> Loss: 0.22819992899894714, epoch: 7350\n",
            "Train-->Loss: 0.2280847132205963, epoch: 7360\n",
            "Test --> Loss: 0.2280847132205963, epoch: 7360\n",
            "Train-->Loss: 0.22796952724456787, epoch: 7370\n",
            "Test --> Loss: 0.22796952724456787, epoch: 7370\n",
            "Train-->Loss: 0.22785432636737823, epoch: 7380\n",
            "Test --> Loss: 0.22785432636737823, epoch: 7380\n",
            "Train-->Loss: 0.2277390956878662, epoch: 7390\n",
            "Test --> Loss: 0.2277390956878662, epoch: 7390\n",
            "Train-->Loss: 0.22762389481067657, epoch: 7400\n",
            "Test --> Loss: 0.22762389481067657, epoch: 7400\n",
            "Train-->Loss: 0.22750866413116455, epoch: 7410\n",
            "Test --> Loss: 0.22750866413116455, epoch: 7410\n",
            "Train-->Loss: 0.22739346325397491, epoch: 7420\n",
            "Test --> Loss: 0.22739346325397491, epoch: 7420\n",
            "Train-->Loss: 0.22727826237678528, epoch: 7430\n",
            "Test --> Loss: 0.22727826237678528, epoch: 7430\n",
            "Train-->Loss: 0.22716303169727325, epoch: 7440\n",
            "Test --> Loss: 0.22716303169727325, epoch: 7440\n",
            "Train-->Loss: 0.22704783082008362, epoch: 7450\n",
            "Test --> Loss: 0.22704783082008362, epoch: 7450\n",
            "Train-->Loss: 0.2269326150417328, epoch: 7460\n",
            "Test --> Loss: 0.2269326150417328, epoch: 7460\n",
            "Train-->Loss: 0.22681741416454315, epoch: 7470\n",
            "Test --> Loss: 0.22681741416454315, epoch: 7470\n",
            "Train-->Loss: 0.22670221328735352, epoch: 7480\n",
            "Test --> Loss: 0.22670221328735352, epoch: 7480\n",
            "Train-->Loss: 0.2265869677066803, epoch: 7490\n",
            "Test --> Loss: 0.2265869677066803, epoch: 7490\n",
            "Train-->Loss: 0.22647175192832947, epoch: 7500\n",
            "Test --> Loss: 0.22647175192832947, epoch: 7500\n",
            "Train-->Loss: 0.22635653614997864, epoch: 7510\n",
            "Test --> Loss: 0.22635653614997864, epoch: 7510\n",
            "Train-->Loss: 0.2262413203716278, epoch: 7520\n",
            "Test --> Loss: 0.2262413203716278, epoch: 7520\n",
            "Train-->Loss: 0.22612611949443817, epoch: 7530\n",
            "Test --> Loss: 0.22612611949443817, epoch: 7530\n",
            "Train-->Loss: 0.22601088881492615, epoch: 7540\n",
            "Test --> Loss: 0.22601088881492615, epoch: 7540\n",
            "Train-->Loss: 0.2258956879377365, epoch: 7550\n",
            "Test --> Loss: 0.2258956879377365, epoch: 7550\n",
            "Train-->Loss: 0.2257804572582245, epoch: 7560\n",
            "Test --> Loss: 0.2257804572582245, epoch: 7560\n",
            "Train-->Loss: 0.22566525638103485, epoch: 7570\n",
            "Test --> Loss: 0.22566525638103485, epoch: 7570\n",
            "Train-->Loss: 0.22555005550384521, epoch: 7580\n",
            "Test --> Loss: 0.22555005550384521, epoch: 7580\n",
            "Train-->Loss: 0.22543485462665558, epoch: 7590\n",
            "Test --> Loss: 0.22543485462665558, epoch: 7590\n",
            "Train-->Loss: 0.22531962394714355, epoch: 7600\n",
            "Test --> Loss: 0.22531962394714355, epoch: 7600\n",
            "Train-->Loss: 0.22520442306995392, epoch: 7610\n",
            "Test --> Loss: 0.22520442306995392, epoch: 7610\n",
            "Train-->Loss: 0.2250891923904419, epoch: 7620\n",
            "Test --> Loss: 0.2250891923904419, epoch: 7620\n",
            "Train-->Loss: 0.22497400641441345, epoch: 7630\n",
            "Test --> Loss: 0.22497400641441345, epoch: 7630\n",
            "Train-->Loss: 0.22485876083374023, epoch: 7640\n",
            "Test --> Loss: 0.22485876083374023, epoch: 7640\n",
            "Train-->Loss: 0.2247435599565506, epoch: 7650\n",
            "Test --> Loss: 0.2247435599565506, epoch: 7650\n",
            "Train-->Loss: 0.22462835907936096, epoch: 7660\n",
            "Test --> Loss: 0.22462835907936096, epoch: 7660\n",
            "Train-->Loss: 0.22451312839984894, epoch: 7670\n",
            "Test --> Loss: 0.22451312839984894, epoch: 7670\n",
            "Train-->Loss: 0.2243979424238205, epoch: 7680\n",
            "Test --> Loss: 0.2243979424238205, epoch: 7680\n",
            "Train-->Loss: 0.22428271174430847, epoch: 7690\n",
            "Test --> Loss: 0.22428271174430847, epoch: 7690\n",
            "Train-->Loss: 0.22416749596595764, epoch: 7700\n",
            "Test --> Loss: 0.22416749596595764, epoch: 7700\n",
            "Train-->Loss: 0.2240522801876068, epoch: 7710\n",
            "Test --> Loss: 0.2240522801876068, epoch: 7710\n",
            "Train-->Loss: 0.22393707931041718, epoch: 7720\n",
            "Test --> Loss: 0.22393707931041718, epoch: 7720\n",
            "Train-->Loss: 0.22382184863090515, epoch: 7730\n",
            "Test --> Loss: 0.22382184863090515, epoch: 7730\n",
            "Train-->Loss: 0.22370664775371552, epoch: 7740\n",
            "Test --> Loss: 0.22370664775371552, epoch: 7740\n",
            "Train-->Loss: 0.22359144687652588, epoch: 7750\n",
            "Test --> Loss: 0.22359144687652588, epoch: 7750\n",
            "Train-->Loss: 0.22347621619701385, epoch: 7760\n",
            "Test --> Loss: 0.22347621619701385, epoch: 7760\n",
            "Train-->Loss: 0.22336098551750183, epoch: 7770\n",
            "Test --> Loss: 0.22336098551750183, epoch: 7770\n",
            "Train-->Loss: 0.22324581444263458, epoch: 7780\n",
            "Test --> Loss: 0.22324581444263458, epoch: 7780\n",
            "Train-->Loss: 0.22313058376312256, epoch: 7790\n",
            "Test --> Loss: 0.22313058376312256, epoch: 7790\n",
            "Train-->Loss: 0.22301535308361053, epoch: 7800\n",
            "Test --> Loss: 0.22301535308361053, epoch: 7800\n",
            "Train-->Loss: 0.2229001522064209, epoch: 7810\n",
            "Test --> Loss: 0.2229001522064209, epoch: 7810\n",
            "Train-->Loss: 0.22278495132923126, epoch: 7820\n",
            "Test --> Loss: 0.22278495132923126, epoch: 7820\n",
            "Train-->Loss: 0.22266975045204163, epoch: 7830\n",
            "Test --> Loss: 0.22266975045204163, epoch: 7830\n",
            "Train-->Loss: 0.2225545197725296, epoch: 7840\n",
            "Test --> Loss: 0.2225545197725296, epoch: 7840\n",
            "Train-->Loss: 0.22243931889533997, epoch: 7850\n",
            "Test --> Loss: 0.22243931889533997, epoch: 7850\n",
            "Train-->Loss: 0.22232410311698914, epoch: 7860\n",
            "Test --> Loss: 0.22232410311698914, epoch: 7860\n",
            "Train-->Loss: 0.2222088873386383, epoch: 7870\n",
            "Test --> Loss: 0.2222088873386383, epoch: 7870\n",
            "Train-->Loss: 0.22209367156028748, epoch: 7880\n",
            "Test --> Loss: 0.22209367156028748, epoch: 7880\n",
            "Train-->Loss: 0.22197845578193665, epoch: 7890\n",
            "Test --> Loss: 0.22197845578193665, epoch: 7890\n",
            "Train-->Loss: 0.22186324000358582, epoch: 7900\n",
            "Test --> Loss: 0.22186324000358582, epoch: 7900\n",
            "Train-->Loss: 0.22174802422523499, epoch: 7910\n",
            "Test --> Loss: 0.22174802422523499, epoch: 7910\n",
            "Train-->Loss: 0.22163280844688416, epoch: 7920\n",
            "Test --> Loss: 0.22163280844688416, epoch: 7920\n",
            "Train-->Loss: 0.22151760756969452, epoch: 7930\n",
            "Test --> Loss: 0.22151760756969452, epoch: 7930\n",
            "Train-->Loss: 0.2214023768901825, epoch: 7940\n",
            "Test --> Loss: 0.2214023768901825, epoch: 7940\n",
            "Train-->Loss: 0.22128716111183167, epoch: 7950\n",
            "Test --> Loss: 0.22128716111183167, epoch: 7950\n",
            "Train-->Loss: 0.22117197513580322, epoch: 7960\n",
            "Test --> Loss: 0.22117197513580322, epoch: 7960\n",
            "Train-->Loss: 0.2210567444562912, epoch: 7970\n",
            "Test --> Loss: 0.2210567444562912, epoch: 7970\n",
            "Train-->Loss: 0.22094154357910156, epoch: 7980\n",
            "Test --> Loss: 0.22094154357910156, epoch: 7980\n",
            "Train-->Loss: 0.22082631289958954, epoch: 7990\n",
            "Test --> Loss: 0.22082631289958954, epoch: 7990\n",
            "Train-->Loss: 0.22071108222007751, epoch: 8000\n",
            "Test --> Loss: 0.22071108222007751, epoch: 8000\n",
            "Train-->Loss: 0.22059591114521027, epoch: 8010\n",
            "Test --> Loss: 0.22059591114521027, epoch: 8010\n",
            "Train-->Loss: 0.22048068046569824, epoch: 8020\n",
            "Test --> Loss: 0.22048068046569824, epoch: 8020\n",
            "Train-->Loss: 0.2203654795885086, epoch: 8030\n",
            "Test --> Loss: 0.2203654795885086, epoch: 8030\n",
            "Train-->Loss: 0.22025027871131897, epoch: 8040\n",
            "Test --> Loss: 0.22025027871131897, epoch: 8040\n",
            "Train-->Loss: 0.22013504803180695, epoch: 8050\n",
            "Test --> Loss: 0.22013504803180695, epoch: 8050\n",
            "Train-->Loss: 0.22001981735229492, epoch: 8060\n",
            "Test --> Loss: 0.22001981735229492, epoch: 8060\n",
            "Train-->Loss: 0.21990461647510529, epoch: 8070\n",
            "Test --> Loss: 0.21990461647510529, epoch: 8070\n",
            "Train-->Loss: 0.21978941559791565, epoch: 8080\n",
            "Test --> Loss: 0.21978941559791565, epoch: 8080\n",
            "Train-->Loss: 0.21967418491840363, epoch: 8090\n",
            "Test --> Loss: 0.21967418491840363, epoch: 8090\n",
            "Train-->Loss: 0.21955899894237518, epoch: 8100\n",
            "Test --> Loss: 0.21955899894237518, epoch: 8100\n",
            "Train-->Loss: 0.21944376826286316, epoch: 8110\n",
            "Test --> Loss: 0.21944376826286316, epoch: 8110\n",
            "Train-->Loss: 0.21932855248451233, epoch: 8120\n",
            "Test --> Loss: 0.21932855248451233, epoch: 8120\n",
            "Train-->Loss: 0.2192133367061615, epoch: 8130\n",
            "Test --> Loss: 0.2192133367061615, epoch: 8130\n",
            "Train-->Loss: 0.21909813582897186, epoch: 8140\n",
            "Test --> Loss: 0.21909813582897186, epoch: 8140\n",
            "Train-->Loss: 0.21898290514945984, epoch: 8150\n",
            "Test --> Loss: 0.21898290514945984, epoch: 8150\n",
            "Train-->Loss: 0.2188677042722702, epoch: 8160\n",
            "Test --> Loss: 0.2188677042722702, epoch: 8160\n",
            "Train-->Loss: 0.21875247359275818, epoch: 8170\n",
            "Test --> Loss: 0.21875247359275818, epoch: 8170\n",
            "Train-->Loss: 0.21863730251789093, epoch: 8180\n",
            "Test --> Loss: 0.21863730251789093, epoch: 8180\n",
            "Train-->Loss: 0.2185220718383789, epoch: 8190\n",
            "Test --> Loss: 0.2185220718383789, epoch: 8190\n",
            "Train-->Loss: 0.21840684115886688, epoch: 8200\n",
            "Test --> Loss: 0.21840684115886688, epoch: 8200\n",
            "Train-->Loss: 0.21829164028167725, epoch: 8210\n",
            "Test --> Loss: 0.21829164028167725, epoch: 8210\n",
            "Train-->Loss: 0.2181764394044876, epoch: 8220\n",
            "Test --> Loss: 0.2181764394044876, epoch: 8220\n",
            "Train-->Loss: 0.21806125342845917, epoch: 8230\n",
            "Test --> Loss: 0.21806125342845917, epoch: 8230\n",
            "Train-->Loss: 0.21794600784778595, epoch: 8240\n",
            "Test --> Loss: 0.21794600784778595, epoch: 8240\n",
            "Train-->Loss: 0.2178308069705963, epoch: 8250\n",
            "Test --> Loss: 0.2178308069705963, epoch: 8250\n",
            "Train-->Loss: 0.2177155762910843, epoch: 8260\n",
            "Test --> Loss: 0.2177155762910843, epoch: 8260\n",
            "Train-->Loss: 0.21760037541389465, epoch: 8270\n",
            "Test --> Loss: 0.21760037541389465, epoch: 8270\n",
            "Train-->Loss: 0.21748515963554382, epoch: 8280\n",
            "Test --> Loss: 0.21748515963554382, epoch: 8280\n",
            "Train-->Loss: 0.217369943857193, epoch: 8290\n",
            "Test --> Loss: 0.217369943857193, epoch: 8290\n",
            "Train-->Loss: 0.21725472807884216, epoch: 8300\n",
            "Test --> Loss: 0.21725472807884216, epoch: 8300\n",
            "Train-->Loss: 0.21713951230049133, epoch: 8310\n",
            "Test --> Loss: 0.21713951230049133, epoch: 8310\n",
            "Train-->Loss: 0.2170243263244629, epoch: 8320\n",
            "Test --> Loss: 0.2170243263244629, epoch: 8320\n",
            "Train-->Loss: 0.21690909564495087, epoch: 8330\n",
            "Test --> Loss: 0.21690909564495087, epoch: 8330\n",
            "Train-->Loss: 0.21679386496543884, epoch: 8340\n",
            "Test --> Loss: 0.21679386496543884, epoch: 8340\n",
            "Train-->Loss: 0.2166786640882492, epoch: 8350\n",
            "Test --> Loss: 0.2166786640882492, epoch: 8350\n",
            "Train-->Loss: 0.21656346321105957, epoch: 8360\n",
            "Test --> Loss: 0.21656346321105957, epoch: 8360\n",
            "Train-->Loss: 0.21644823253154755, epoch: 8370\n",
            "Test --> Loss: 0.21644823253154755, epoch: 8370\n",
            "Train-->Loss: 0.2163330316543579, epoch: 8380\n",
            "Test --> Loss: 0.2163330316543579, epoch: 8380\n",
            "Train-->Loss: 0.2162178009748459, epoch: 8390\n",
            "Test --> Loss: 0.2162178009748459, epoch: 8390\n",
            "Train-->Loss: 0.21610260009765625, epoch: 8400\n",
            "Test --> Loss: 0.21610260009765625, epoch: 8400\n",
            "Train-->Loss: 0.2159873992204666, epoch: 8410\n",
            "Test --> Loss: 0.2159873992204666, epoch: 8410\n",
            "Train-->Loss: 0.2158721685409546, epoch: 8420\n",
            "Test --> Loss: 0.2158721685409546, epoch: 8420\n",
            "Train-->Loss: 0.21575698256492615, epoch: 8430\n",
            "Test --> Loss: 0.21575698256492615, epoch: 8430\n",
            "Train-->Loss: 0.21564173698425293, epoch: 8440\n",
            "Test --> Loss: 0.21564173698425293, epoch: 8440\n",
            "Train-->Loss: 0.2155265361070633, epoch: 8450\n",
            "Test --> Loss: 0.2155265361070633, epoch: 8450\n",
            "Train-->Loss: 0.21541133522987366, epoch: 8460\n",
            "Test --> Loss: 0.21541133522987366, epoch: 8460\n",
            "Train-->Loss: 0.21529610455036163, epoch: 8470\n",
            "Test --> Loss: 0.21529610455036163, epoch: 8470\n",
            "Train-->Loss: 0.215180903673172, epoch: 8480\n",
            "Test --> Loss: 0.215180903673172, epoch: 8480\n",
            "Train-->Loss: 0.21506568789482117, epoch: 8490\n",
            "Test --> Loss: 0.21506568789482117, epoch: 8490\n",
            "Train-->Loss: 0.21495048701763153, epoch: 8500\n",
            "Test --> Loss: 0.21495048701763153, epoch: 8500\n",
            "Train-->Loss: 0.2148352563381195, epoch: 8510\n",
            "Test --> Loss: 0.2148352563381195, epoch: 8510\n",
            "Train-->Loss: 0.21472004055976868, epoch: 8520\n",
            "Test --> Loss: 0.21472004055976868, epoch: 8520\n",
            "Train-->Loss: 0.21460482478141785, epoch: 8530\n",
            "Test --> Loss: 0.21460482478141785, epoch: 8530\n",
            "Train-->Loss: 0.2144896239042282, epoch: 8540\n",
            "Test --> Loss: 0.2144896239042282, epoch: 8540\n",
            "Train-->Loss: 0.2143743932247162, epoch: 8550\n",
            "Test --> Loss: 0.2143743932247162, epoch: 8550\n",
            "Train-->Loss: 0.21425919234752655, epoch: 8560\n",
            "Test --> Loss: 0.21425919234752655, epoch: 8560\n",
            "Train-->Loss: 0.21414399147033691, epoch: 8570\n",
            "Test --> Loss: 0.21414399147033691, epoch: 8570\n",
            "Train-->Loss: 0.2140287607908249, epoch: 8580\n",
            "Test --> Loss: 0.2140287607908249, epoch: 8580\n",
            "Train-->Loss: 0.21391355991363525, epoch: 8590\n",
            "Test --> Loss: 0.21391355991363525, epoch: 8590\n",
            "Train-->Loss: 0.21379835903644562, epoch: 8600\n",
            "Test --> Loss: 0.21379835903644562, epoch: 8600\n",
            "Train-->Loss: 0.2136831283569336, epoch: 8610\n",
            "Test --> Loss: 0.2136831283569336, epoch: 8610\n",
            "Train-->Loss: 0.21356792747974396, epoch: 8620\n",
            "Test --> Loss: 0.21356792747974396, epoch: 8620\n",
            "Train-->Loss: 0.21345272660255432, epoch: 8630\n",
            "Test --> Loss: 0.21345272660255432, epoch: 8630\n",
            "Train-->Loss: 0.2133375108242035, epoch: 8640\n",
            "Test --> Loss: 0.2133375108242035, epoch: 8640\n",
            "Train-->Loss: 0.21322229504585266, epoch: 8650\n",
            "Test --> Loss: 0.21322229504585266, epoch: 8650\n",
            "Train-->Loss: 0.21310706436634064, epoch: 8660\n",
            "Test --> Loss: 0.21310706436634064, epoch: 8660\n",
            "Train-->Loss: 0.212991863489151, epoch: 8670\n",
            "Test --> Loss: 0.212991863489151, epoch: 8670\n",
            "Train-->Loss: 0.21287664771080017, epoch: 8680\n",
            "Test --> Loss: 0.21287664771080017, epoch: 8680\n",
            "Train-->Loss: 0.21276143193244934, epoch: 8690\n",
            "Test --> Loss: 0.21276143193244934, epoch: 8690\n",
            "Train-->Loss: 0.21264620125293732, epoch: 8700\n",
            "Test --> Loss: 0.21264620125293732, epoch: 8700\n",
            "Train-->Loss: 0.21253100037574768, epoch: 8710\n",
            "Test --> Loss: 0.21253100037574768, epoch: 8710\n",
            "Train-->Loss: 0.21241578459739685, epoch: 8720\n",
            "Test --> Loss: 0.21241578459739685, epoch: 8720\n",
            "Train-->Loss: 0.21230056881904602, epoch: 8730\n",
            "Test --> Loss: 0.21230056881904602, epoch: 8730\n",
            "Train-->Loss: 0.2121853530406952, epoch: 8740\n",
            "Test --> Loss: 0.2121853530406952, epoch: 8740\n",
            "Train-->Loss: 0.21207015216350555, epoch: 8750\n",
            "Test --> Loss: 0.21207015216350555, epoch: 8750\n",
            "Train-->Loss: 0.21195492148399353, epoch: 8760\n",
            "Test --> Loss: 0.21195492148399353, epoch: 8760\n",
            "Train-->Loss: 0.2118397206068039, epoch: 8770\n",
            "Test --> Loss: 0.2118397206068039, epoch: 8770\n",
            "Train-->Loss: 0.21172451972961426, epoch: 8780\n",
            "Test --> Loss: 0.21172451972961426, epoch: 8780\n",
            "Train-->Loss: 0.21160928905010223, epoch: 8790\n",
            "Test --> Loss: 0.21160928905010223, epoch: 8790\n",
            "Train-->Loss: 0.2114940583705902, epoch: 8800\n",
            "Test --> Loss: 0.2114940583705902, epoch: 8800\n",
            "Train-->Loss: 0.21137888729572296, epoch: 8810\n",
            "Test --> Loss: 0.21137888729572296, epoch: 8810\n",
            "Train-->Loss: 0.21126365661621094, epoch: 8820\n",
            "Test --> Loss: 0.21126365661621094, epoch: 8820\n",
            "Train-->Loss: 0.2111484557390213, epoch: 8830\n",
            "Test --> Loss: 0.2111484557390213, epoch: 8830\n",
            "Train-->Loss: 0.21103322505950928, epoch: 8840\n",
            "Test --> Loss: 0.21103322505950928, epoch: 8840\n",
            "Train-->Loss: 0.21091802418231964, epoch: 8850\n",
            "Test --> Loss: 0.21091802418231964, epoch: 8850\n",
            "Train-->Loss: 0.21080282330513, epoch: 8860\n",
            "Test --> Loss: 0.21080282330513, epoch: 8860\n",
            "Train-->Loss: 0.21068760752677917, epoch: 8870\n",
            "Test --> Loss: 0.21068760752677917, epoch: 8870\n",
            "Train-->Loss: 0.21057239174842834, epoch: 8880\n",
            "Test --> Loss: 0.21057239174842834, epoch: 8880\n",
            "Train-->Loss: 0.21045717597007751, epoch: 8890\n",
            "Test --> Loss: 0.21045717597007751, epoch: 8890\n",
            "Train-->Loss: 0.21034196019172668, epoch: 8900\n",
            "Test --> Loss: 0.21034196019172668, epoch: 8900\n",
            "Train-->Loss: 0.21022672951221466, epoch: 8910\n",
            "Test --> Loss: 0.21022672951221466, epoch: 8910\n",
            "Train-->Loss: 0.21011154353618622, epoch: 8920\n",
            "Test --> Loss: 0.21011154353618622, epoch: 8920\n",
            "Train-->Loss: 0.2099963128566742, epoch: 8930\n",
            "Test --> Loss: 0.2099963128566742, epoch: 8930\n",
            "Train-->Loss: 0.20988111197948456, epoch: 8940\n",
            "Test --> Loss: 0.20988111197948456, epoch: 8940\n",
            "Train-->Loss: 0.20976588129997253, epoch: 8950\n",
            "Test --> Loss: 0.20976588129997253, epoch: 8950\n",
            "Train-->Loss: 0.2096506655216217, epoch: 8960\n",
            "Test --> Loss: 0.2096506655216217, epoch: 8960\n",
            "Train-->Loss: 0.20953547954559326, epoch: 8970\n",
            "Test --> Loss: 0.20953547954559326, epoch: 8970\n",
            "Train-->Loss: 0.20942027866840363, epoch: 8980\n",
            "Test --> Loss: 0.20942027866840363, epoch: 8980\n",
            "Train-->Loss: 0.2093050479888916, epoch: 8990\n",
            "Test --> Loss: 0.2093050479888916, epoch: 8990\n",
            "Train-->Loss: 0.20918984711170197, epoch: 9000\n",
            "Test --> Loss: 0.20918984711170197, epoch: 9000\n",
            "Train-->Loss: 0.20907461643218994, epoch: 9010\n",
            "Test --> Loss: 0.20907461643218994, epoch: 9010\n",
            "Train-->Loss: 0.2089594155550003, epoch: 9020\n",
            "Test --> Loss: 0.2089594155550003, epoch: 9020\n",
            "Train-->Loss: 0.20884421467781067, epoch: 9030\n",
            "Test --> Loss: 0.20884421467781067, epoch: 9030\n",
            "Train-->Loss: 0.20872898399829865, epoch: 9040\n",
            "Test --> Loss: 0.20872898399829865, epoch: 9040\n",
            "Train-->Loss: 0.208613783121109, epoch: 9050\n",
            "Test --> Loss: 0.208613783121109, epoch: 9050\n",
            "Train-->Loss: 0.20849856734275818, epoch: 9060\n",
            "Test --> Loss: 0.20849856734275818, epoch: 9060\n",
            "Train-->Loss: 0.20838335156440735, epoch: 9070\n",
            "Test --> Loss: 0.20838335156440735, epoch: 9070\n",
            "Train-->Loss: 0.20826812088489532, epoch: 9080\n",
            "Test --> Loss: 0.20826812088489532, epoch: 9080\n",
            "Train-->Loss: 0.2081529200077057, epoch: 9090\n",
            "Test --> Loss: 0.2081529200077057, epoch: 9090\n",
            "Train-->Loss: 0.20803770422935486, epoch: 9100\n",
            "Test --> Loss: 0.20803770422935486, epoch: 9100\n",
            "Train-->Loss: 0.20792248845100403, epoch: 9110\n",
            "Test --> Loss: 0.20792248845100403, epoch: 9110\n",
            "Train-->Loss: 0.2078072726726532, epoch: 9120\n",
            "Test --> Loss: 0.2078072726726532, epoch: 9120\n",
            "Train-->Loss: 0.20769207179546356, epoch: 9130\n",
            "Test --> Loss: 0.20769207179546356, epoch: 9130\n",
            "Train-->Loss: 0.20757684111595154, epoch: 9140\n",
            "Test --> Loss: 0.20757684111595154, epoch: 9140\n",
            "Train-->Loss: 0.2074616402387619, epoch: 9150\n",
            "Test --> Loss: 0.2074616402387619, epoch: 9150\n",
            "Train-->Loss: 0.20734640955924988, epoch: 9160\n",
            "Test --> Loss: 0.20734640955924988, epoch: 9160\n",
            "Train-->Loss: 0.20723120868206024, epoch: 9170\n",
            "Test --> Loss: 0.20723120868206024, epoch: 9170\n",
            "Train-->Loss: 0.2071160078048706, epoch: 9180\n",
            "Test --> Loss: 0.2071160078048706, epoch: 9180\n",
            "Train-->Loss: 0.20700080692768097, epoch: 9190\n",
            "Test --> Loss: 0.20700080692768097, epoch: 9190\n",
            "Train-->Loss: 0.20688557624816895, epoch: 9200\n",
            "Test --> Loss: 0.20688557624816895, epoch: 9200\n",
            "Train-->Loss: 0.2067703753709793, epoch: 9210\n",
            "Test --> Loss: 0.2067703753709793, epoch: 9210\n",
            "Train-->Loss: 0.20665514469146729, epoch: 9220\n",
            "Test --> Loss: 0.20665514469146729, epoch: 9220\n",
            "Train-->Loss: 0.20653991401195526, epoch: 9230\n",
            "Test --> Loss: 0.20653991401195526, epoch: 9230\n",
            "Train-->Loss: 0.20642471313476562, epoch: 9240\n",
            "Test --> Loss: 0.20642471313476562, epoch: 9240\n",
            "Train-->Loss: 0.206309512257576, epoch: 9250\n",
            "Test --> Loss: 0.206309512257576, epoch: 9250\n",
            "Train-->Loss: 0.20619431138038635, epoch: 9260\n",
            "Test --> Loss: 0.20619431138038635, epoch: 9260\n",
            "Train-->Loss: 0.20607908070087433, epoch: 9270\n",
            "Test --> Loss: 0.20607908070087433, epoch: 9270\n",
            "Train-->Loss: 0.2059638947248459, epoch: 9280\n",
            "Test --> Loss: 0.2059638947248459, epoch: 9280\n",
            "Train-->Loss: 0.20584866404533386, epoch: 9290\n",
            "Test --> Loss: 0.20584866404533386, epoch: 9290\n",
            "Train-->Loss: 0.20573344826698303, epoch: 9300\n",
            "Test --> Loss: 0.20573344826698303, epoch: 9300\n",
            "Train-->Loss: 0.2056182324886322, epoch: 9310\n",
            "Test --> Loss: 0.2056182324886322, epoch: 9310\n",
            "Train-->Loss: 0.20550303161144257, epoch: 9320\n",
            "Test --> Loss: 0.20550303161144257, epoch: 9320\n",
            "Train-->Loss: 0.20538780093193054, epoch: 9330\n",
            "Test --> Loss: 0.20538780093193054, epoch: 9330\n",
            "Train-->Loss: 0.2052726000547409, epoch: 9340\n",
            "Test --> Loss: 0.2052726000547409, epoch: 9340\n",
            "Train-->Loss: 0.20515739917755127, epoch: 9350\n",
            "Test --> Loss: 0.20515739917755127, epoch: 9350\n",
            "Train-->Loss: 0.20504216849803925, epoch: 9360\n",
            "Test --> Loss: 0.20504216849803925, epoch: 9360\n",
            "Train-->Loss: 0.20492693781852722, epoch: 9370\n",
            "Test --> Loss: 0.20492693781852722, epoch: 9370\n",
            "Train-->Loss: 0.20481176674365997, epoch: 9380\n",
            "Test --> Loss: 0.20481176674365997, epoch: 9380\n",
            "Train-->Loss: 0.20469653606414795, epoch: 9390\n",
            "Test --> Loss: 0.20469653606414795, epoch: 9390\n",
            "Train-->Loss: 0.20458130538463593, epoch: 9400\n",
            "Test --> Loss: 0.20458130538463593, epoch: 9400\n",
            "Train-->Loss: 0.2044661045074463, epoch: 9410\n",
            "Test --> Loss: 0.2044661045074463, epoch: 9410\n",
            "Train-->Loss: 0.20435090363025665, epoch: 9420\n",
            "Test --> Loss: 0.20435090363025665, epoch: 9420\n",
            "Train-->Loss: 0.20423570275306702, epoch: 9430\n",
            "Test --> Loss: 0.20423570275306702, epoch: 9430\n",
            "Train-->Loss: 0.204120472073555, epoch: 9440\n",
            "Test --> Loss: 0.204120472073555, epoch: 9440\n",
            "Train-->Loss: 0.20400527119636536, epoch: 9450\n",
            "Test --> Loss: 0.20400527119636536, epoch: 9450\n",
            "Train-->Loss: 0.20389005541801453, epoch: 9460\n",
            "Test --> Loss: 0.20389005541801453, epoch: 9460\n",
            "Train-->Loss: 0.2037748396396637, epoch: 9470\n",
            "Test --> Loss: 0.2037748396396637, epoch: 9470\n",
            "Train-->Loss: 0.20365960896015167, epoch: 9480\n",
            "Test --> Loss: 0.20365960896015167, epoch: 9480\n",
            "Train-->Loss: 0.20354440808296204, epoch: 9490\n",
            "Test --> Loss: 0.20354440808296204, epoch: 9490\n",
            "Train-->Loss: 0.2034291923046112, epoch: 9500\n",
            "Test --> Loss: 0.2034291923046112, epoch: 9500\n",
            "Train-->Loss: 0.20331397652626038, epoch: 9510\n",
            "Test --> Loss: 0.20331397652626038, epoch: 9510\n",
            "Train-->Loss: 0.20319876074790955, epoch: 9520\n",
            "Test --> Loss: 0.20319876074790955, epoch: 9520\n",
            "Train-->Loss: 0.2030835598707199, epoch: 9530\n",
            "Test --> Loss: 0.2030835598707199, epoch: 9530\n",
            "Train-->Loss: 0.20296832919120789, epoch: 9540\n",
            "Test --> Loss: 0.20296832919120789, epoch: 9540\n",
            "Train-->Loss: 0.20285311341285706, epoch: 9550\n",
            "Test --> Loss: 0.20285311341285706, epoch: 9550\n",
            "Train-->Loss: 0.2027379274368286, epoch: 9560\n",
            "Test --> Loss: 0.2027379274368286, epoch: 9560\n",
            "Train-->Loss: 0.2026226967573166, epoch: 9570\n",
            "Test --> Loss: 0.2026226967573166, epoch: 9570\n",
            "Train-->Loss: 0.20250749588012695, epoch: 9580\n",
            "Test --> Loss: 0.20250749588012695, epoch: 9580\n",
            "Train-->Loss: 0.20239226520061493, epoch: 9590\n",
            "Test --> Loss: 0.20239226520061493, epoch: 9590\n",
            "Train-->Loss: 0.2022770345211029, epoch: 9600\n",
            "Test --> Loss: 0.2022770345211029, epoch: 9600\n",
            "Train-->Loss: 0.20216186344623566, epoch: 9610\n",
            "Test --> Loss: 0.20216186344623566, epoch: 9610\n",
            "Train-->Loss: 0.20204663276672363, epoch: 9620\n",
            "Test --> Loss: 0.20204663276672363, epoch: 9620\n",
            "Train-->Loss: 0.201931431889534, epoch: 9630\n",
            "Test --> Loss: 0.201931431889534, epoch: 9630\n",
            "Train-->Loss: 0.20181623101234436, epoch: 9640\n",
            "Test --> Loss: 0.20181623101234436, epoch: 9640\n",
            "Train-->Loss: 0.20170100033283234, epoch: 9650\n",
            "Test --> Loss: 0.20170100033283234, epoch: 9650\n",
            "Train-->Loss: 0.2015857696533203, epoch: 9660\n",
            "Test --> Loss: 0.2015857696533203, epoch: 9660\n",
            "Train-->Loss: 0.20147056877613068, epoch: 9670\n",
            "Test --> Loss: 0.20147056877613068, epoch: 9670\n",
            "Train-->Loss: 0.20135536789894104, epoch: 9680\n",
            "Test --> Loss: 0.20135536789894104, epoch: 9680\n",
            "Train-->Loss: 0.20124013721942902, epoch: 9690\n",
            "Test --> Loss: 0.20124013721942902, epoch: 9690\n",
            "Train-->Loss: 0.20112495124340057, epoch: 9700\n",
            "Test --> Loss: 0.20112495124340057, epoch: 9700\n",
            "Train-->Loss: 0.20100972056388855, epoch: 9710\n",
            "Test --> Loss: 0.20100972056388855, epoch: 9710\n",
            "Train-->Loss: 0.20089450478553772, epoch: 9720\n",
            "Test --> Loss: 0.20089450478553772, epoch: 9720\n",
            "Train-->Loss: 0.2007792890071869, epoch: 9730\n",
            "Test --> Loss: 0.2007792890071869, epoch: 9730\n",
            "Train-->Loss: 0.20066408812999725, epoch: 9740\n",
            "Test --> Loss: 0.20066408812999725, epoch: 9740\n",
            "Train-->Loss: 0.20054885745048523, epoch: 9750\n",
            "Test --> Loss: 0.20054885745048523, epoch: 9750\n",
            "Train-->Loss: 0.2004336565732956, epoch: 9760\n",
            "Test --> Loss: 0.2004336565732956, epoch: 9760\n",
            "Train-->Loss: 0.20031845569610596, epoch: 9770\n",
            "Test --> Loss: 0.20031845569610596, epoch: 9770\n",
            "Train-->Loss: 0.20020325481891632, epoch: 9780\n",
            "Test --> Loss: 0.20020325481891632, epoch: 9780\n",
            "Train-->Loss: 0.2000880241394043, epoch: 9790\n",
            "Test --> Loss: 0.2000880241394043, epoch: 9790\n",
            "Train-->Loss: 0.19997279345989227, epoch: 9800\n",
            "Test --> Loss: 0.19997279345989227, epoch: 9800\n",
            "Train-->Loss: 0.19985757768154144, epoch: 9810\n",
            "Test --> Loss: 0.19985757768154144, epoch: 9810\n",
            "Train-->Loss: 0.199742391705513, epoch: 9820\n",
            "Test --> Loss: 0.199742391705513, epoch: 9820\n",
            "Train-->Loss: 0.19962719082832336, epoch: 9830\n",
            "Test --> Loss: 0.19962719082832336, epoch: 9830\n",
            "Train-->Loss: 0.19951196014881134, epoch: 9840\n",
            "Test --> Loss: 0.19951196014881134, epoch: 9840\n",
            "Train-->Loss: 0.1993967592716217, epoch: 9850\n",
            "Test --> Loss: 0.1993967592716217, epoch: 9850\n",
            "Train-->Loss: 0.19928152859210968, epoch: 9860\n",
            "Test --> Loss: 0.19928152859210968, epoch: 9860\n",
            "Train-->Loss: 0.19916632771492004, epoch: 9870\n",
            "Test --> Loss: 0.19916632771492004, epoch: 9870\n",
            "Train-->Loss: 0.1990511119365692, epoch: 9880\n",
            "Test --> Loss: 0.1990511119365692, epoch: 9880\n",
            "Train-->Loss: 0.1989358812570572, epoch: 9890\n",
            "Test --> Loss: 0.1989358812570572, epoch: 9890\n",
            "Train-->Loss: 0.19882068037986755, epoch: 9900\n",
            "Test --> Loss: 0.19882068037986755, epoch: 9900\n",
            "Train-->Loss: 0.19870546460151672, epoch: 9910\n",
            "Test --> Loss: 0.19870546460151672, epoch: 9910\n",
            "Train-->Loss: 0.1985902488231659, epoch: 9920\n",
            "Test --> Loss: 0.1985902488231659, epoch: 9920\n",
            "Train-->Loss: 0.19847504794597626, epoch: 9930\n",
            "Test --> Loss: 0.19847504794597626, epoch: 9930\n",
            "Train-->Loss: 0.19835981726646423, epoch: 9940\n",
            "Test --> Loss: 0.19835981726646423, epoch: 9940\n",
            "Train-->Loss: 0.1982446163892746, epoch: 9950\n",
            "Test --> Loss: 0.1982446163892746, epoch: 9950\n",
            "Train-->Loss: 0.19812941551208496, epoch: 9960\n",
            "Test --> Loss: 0.19812941551208496, epoch: 9960\n",
            "Train-->Loss: 0.19801418483257294, epoch: 9970\n",
            "Test --> Loss: 0.19801418483257294, epoch: 9970\n",
            "Train-->Loss: 0.1978989839553833, epoch: 9980\n",
            "Test --> Loss: 0.1978989839553833, epoch: 9980\n",
            "Train-->Loss: 0.19778376817703247, epoch: 9990\n",
            "Test --> Loss: 0.19778376817703247, epoch: 9990\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3757])), ('bias', tensor([0.2288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfQ82sGndgR9",
        "outputId": "fdb06b8f-3edb-4b98-a238-1ac825aa844f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib as plt\n",
        "import matplotlib.pyplot as plt\n",
        "torch.tensor(epoch_count).numpy(),torch.tensor(loss_values).numpy(),torch.tensor(test_loss_value).numpy()\n",
        "plt.plot(torch.tensor(epoch_count).numpy(),torch.tensor(loss_values).numpy(),label=\"Train Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "7NxnfCyxbZ1e",
        "outputId": "1b6988a5-a628-4dd8-c1a5-1c88847318b2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}